{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Installation and Imports"
      ],
      "metadata": {
        "id": "ak1X6bohRzkN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q \"transformers>=4.46.0\" \"accelerate>=1.1.0\" peft datasets\n"
      ],
      "metadata": {
        "id": "c1PKznkVBqY4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install -q \\\n",
        "    torch  \\\n",
        "    datasets \\\n",
        "    accelerate \\\n",
        "    numpy \\\n",
        "    pandas \\\n",
        "    scikit-learn \\\n",
        "    matplotlib \\\n",
        "    seaborn \\\n",
        "    wordcloud \\\n",
        "    emoji \\\n",
        "    nltk \\\n",
        "    shap \\\n",
        "    lime"
      ],
      "metadata": {
        "id": "WttI-0Hn172m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os, re, sys, random, unicodedata\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from IPython.display import display\n",
        "from wordcloud import WordCloud\n",
        "\n",
        "# NLP\n",
        "import emoji, nltk\n",
        "from nltk import word_tokenize, pos_tag\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import wordnet\n",
        "for pkg in [\"punkt\",\"punkt_tab\",\"averaged_perceptron_tagger\",\"averaged_perceptron_tagger_eng\",\"wordnet\",\"omw-1.4\"]:\n",
        "    nltk.download(pkg, quiet=True)\n",
        "\n",
        "# ML\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.decomposition import PCA, TruncatedSVD\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from sklearn.metrics import (\n",
        "    classification_report, confusion_matrix,\n",
        "    roc_auc_score, roc_curve,\n",
        "    accuracy_score, precision_recall_fscore_support\n",
        ")\n",
        "\n",
        "from scipy.sparse import hstack, csr_matrix\n",
        "# SHAP + LIME\n",
        "import shap\n",
        "from lime.lime_text import LimeTextExplainer\n",
        "from scipy.special import softmax\n",
        "\n",
        "# TORCH + TRANSFORMERS + PEFT\n",
        "import torch\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForSequenceClassification,\n",
        "    DataCollatorWithPadding,\n",
        "    TrainingArguments,\n",
        "    AutoModelForCausalLM,\n",
        "    DataCollatorForLanguageModeling,\n",
        ")\n",
        "\n",
        "from peft import LoraConfig, get_peft_model, TaskType\n",
        "\n",
        "# GLOBAL SEED\n",
        "RANDOM_SEED = 42\n",
        "random.seed(RANDOM_SEED)\n",
        "np.random.seed(RANDOM_SEED)\n",
        "\n",
        "print(\" SETUP COMPLETE\")\n",
        "print(\"PyTorch:\", torch.__version__)"
      ],
      "metadata": {
        "id": "1Cwq7jCw2jQA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Initial Data Loading and Cleaning"
      ],
      "metadata": {
        "id": "oTZHHkCWR7Dd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def percent_to_float(x):\n",
        "    if pd.isna(x):\n",
        "        return np.nan\n",
        "    if isinstance(x, str):\n",
        "        x = x.strip()\n",
        "        if x.endswith(\"%\"):\n",
        "            x = x[:-1]\n",
        "        try:\n",
        "            return float(x)\n",
        "        except:\n",
        "            return np.nan\n",
        "    try:\n",
        "        return float(x)\n",
        "    except:\n",
        "        return np.nan\n",
        "\n",
        "\n",
        "def drop_unnamed(df):\n",
        "    return df.loc[:, ~df.columns.str.startswith(\"Unnamed:\")]\n",
        "\n",
        "def load_and_clean_raw_datasets(base_path=\".\"):\n",
        "    rmp = pd.read_csv(os.path.join(base_path,\n",
        "                     \"1_He_2020_RMP_v2_with_sampling_criteria.csv\"),\n",
        "                     encoding=\"latin1\", low_memory=False)\n",
        "    waterloo = pd.read_csv(os.path.join(base_path,\n",
        "                     \"2_Uwaterloo_course_reviews_with_sampling_criteria.csv\"),\n",
        "                     encoding=\"latin1\", low_memory=False)\n",
        "    exeter = pd.read_csv(os.path.join(base_path,\n",
        "                     \"3_UExeter_uni_reviews_with_sampling_criteria.csv\"),\n",
        "                     encoding=\"latin1\", low_memory=False)\n",
        "\n",
        "    print(\"Raw shapes:\", rmp.shape, waterloo.shape, exeter.shape)\n",
        "\n",
        "    rmp = drop_unnamed(rmp)\n",
        "    waterloo = drop_unnamed(waterloo)\n",
        "    exeter = drop_unnamed(exeter)\n",
        "\n",
        "    for df in [rmp, waterloo, exeter]:\n",
        "        if \"num_token\" in df.columns:\n",
        "            df[\"num_token\"] = pd.to_numeric(df[\"num_token\"], errors=\"coerce\")\n",
        "\n",
        "    if \"rating_cat\" in rmp.columns:\n",
        "        rmp = rmp[rmp[\"rating_cat\"].isin([\"Pos\", \"Neg\", \"Conf/Neu\"])]\n",
        "\n",
        "    if \"rating_cat\" in waterloo.columns:\n",
        "        waterloo.loc[waterloo[\"rating_cat\"] == \"[NA]\", \"rating_cat\"] = np.nan\n",
        "        waterloo = waterloo.dropna(subset=[\"rating_cat\"])\n",
        "\n",
        "    for col in [\"useful\", \"easy\", \"liked\"]:\n",
        "        if col in waterloo.columns:\n",
        "            waterloo[col] = waterloo[col].apply(percent_to_float)\n",
        "\n",
        "    rmp = rmp.reset_index(drop=True)\n",
        "    waterloo = waterloo.reset_index(drop=True)\n",
        "    exeter = exeter.reset_index(drop=True)\n",
        "\n",
        "    print(\"After cleaning:\", rmp.shape, waterloo.shape, exeter.shape)\n",
        "\n",
        "    return rmp, waterloo, exeter\n",
        "\n",
        "\n",
        "df_rmp, df_waterloo, df_exeter = load_and_clean_raw_datasets()\n"
      ],
      "metadata": {
        "id": "geSqgPJf2dhs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Unified schema + merged df_all (teacher / course / university)"
      ],
      "metadata": {
        "id": "_vKFXCzDJwJv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "UNIFIED_COLS = [\n",
        "    \"global_id\",\n",
        "    \"dataset\",\n",
        "    \"entity_type\",\n",
        "    \"source_id\",\n",
        "    \"review_text\",\n",
        "    \"rating\",\n",
        "    \"rating_bin\",\n",
        "    \"rating_cat\",\n",
        "    \"num_token\",\n",
        "    \"length_bin\",\n",
        "    \"sample_criteria\",\n",
        "    \"department\",\n",
        "    \"teacher_name\",\n",
        "    \"course_code\",\n",
        "    \"course_name\",\n",
        "    \"num_ratings\",\n",
        "    \"num_reviews\",\n",
        "    \"useful\",\n",
        "    \"easy\",\n",
        "    \"liked\",\n",
        "    \"student_id\",\n",
        "    \"date\",\n",
        "    \"rating_facilities\",\n",
        "    \"rating_clubs\",\n",
        "    \"rating_careerService\",\n",
        "    \"rating_internet\",\n",
        "]\n",
        "\n",
        "DEFAULTS = {\n",
        "    \"global_id\": \"\",\n",
        "    \"dataset\": \"\",\n",
        "    \"entity_type\": \"\",\n",
        "    \"source_id\": \"\",\n",
        "    \"review_text\": \"\",\n",
        "    \"rating\": 0.0,\n",
        "    \"rating_bin\": 0,\n",
        "    \"rating_cat\": \"unknown\",\n",
        "    \"num_token\": 0,\n",
        "    \"length_bin\": 0,\n",
        "    \"sample_criteria\": \"unknown\",\n",
        "    \"department\": \"\",\n",
        "    \"teacher_name\": \"\",\n",
        "    \"course_code\": \"\",\n",
        "    \"course_name\": \"\",\n",
        "    \"num_ratings\": 0,\n",
        "    \"num_reviews\": 0,\n",
        "    \"useful\": 0.0,\n",
        "    \"easy\": 0.0,\n",
        "    \"liked\": 0.0,\n",
        "    \"student_id\": \"\",\n",
        "    \"date\": \"\",\n",
        "    \"rating_facilities\": 0.0,\n",
        "    \"rating_clubs\": 0.0,\n",
        "    \"rating_careerService\": 0.0,\n",
        "    \"rating_internet\": 0.0,\n",
        "}\n",
        "\n",
        "\n",
        "def fill_missing_cols(df, cols, defaults):\n",
        "    for c in cols:\n",
        "        if c not in df.columns:\n",
        "            df[c] = defaults.get(c, \"\")\n",
        "    return df[cols]\n",
        "\n",
        "\n",
        "def build_unified_df(df_rmp, df_waterloo, df_exeter):\n",
        "    # RMP (teacher)\n",
        "    rmp_u = pd.DataFrame(index=df_rmp.index)\n",
        "    rmp_u[\"dataset\"] = \"rmp\"\n",
        "    rmp_u[\"entity_type\"] = \"teacher\"\n",
        "    rmp_u[\"source_id\"] = df_rmp[\"id\"]\n",
        "    rmp_u[\"global_id\"] = \"rmp_\" + df_rmp[\"id\"].astype(str)\n",
        "    rmp_u[\"review_text\"] = df_rmp[\"review_text\"]\n",
        "    rmp_u[\"rating\"] = df_rmp[\"rating\"]\n",
        "    rmp_u[\"rating_bin\"] = df_rmp[\"rating_bin\"]\n",
        "    rmp_u[\"rating_cat\"] = df_rmp[\"rating_cat\"]\n",
        "    rmp_u[\"num_token\"] = df_rmp[\"num_token\"]\n",
        "    rmp_u[\"length_bin\"] = df_rmp[\"length_bin\"]\n",
        "    rmp_u[\"sample_criteria\"] = df_rmp[\"sample_criteria\"]\n",
        "    rmp_u[\"department\"] = df_rmp[\"department\"]\n",
        "    rmp_u[\"teacher_name\"] = df_rmp[\"teacher_name\"]\n",
        "    rmp_u = fill_missing_cols(rmp_u, UNIFIED_COLS, DEFAULTS)\n",
        "\n",
        "    # Waterloo (course)\n",
        "    waterloo_u = pd.DataFrame(index=df_waterloo.index)\n",
        "    waterloo_u[\"dataset\"] = \"waterloo\"\n",
        "    waterloo_u[\"entity_type\"] = \"course\"\n",
        "    waterloo_u[\"source_id\"] = df_waterloo[\"id\"]\n",
        "    waterloo_u[\"global_id\"] = \"waterloo_\" + df_waterloo[\"id\"].astype(str)\n",
        "    waterloo_u[\"review_text\"] = df_waterloo[\"review_text\"]\n",
        "    waterloo_u[\"rating\"] = df_waterloo[\"rating\"]\n",
        "    waterloo_u[\"rating_bin\"] = df_waterloo[\"rating_bin\"]\n",
        "    waterloo_u[\"rating_cat\"] = df_waterloo[\"rating_cat\"]\n",
        "    waterloo_u[\"num_token\"] = df_waterloo.get(\"num_token\", 0)\n",
        "    waterloo_u[\"length_bin\"] = df_waterloo.get(\"length_bin\", 0)\n",
        "    waterloo_u[\"sample_criteria\"] = df_waterloo.get(\"sample_criteria\", \"unknown\")\n",
        "    waterloo_u[\"course_code\"] = df_waterloo.get(\"course_code\", \"\")\n",
        "    waterloo_u[\"course_name\"] = df_waterloo.get(\"course_id\", \"\")\n",
        "    waterloo_u[\"num_ratings\"] = df_waterloo.get(\"num_ratings\", 0)\n",
        "    waterloo_u[\"num_reviews\"] = df_waterloo.get(\"num_reviews\", 0)\n",
        "    waterloo_u[\"useful\"] = df_waterloo.get(\"useful\", 0.0)\n",
        "    waterloo_u[\"easy\"] = df_waterloo.get(\"easy\", 0.0)\n",
        "    waterloo_u[\"liked\"] = df_waterloo.get(\"liked\", 0.0)\n",
        "    waterloo_u = fill_missing_cols(waterloo_u, UNIFIED_COLS, DEFAULTS)\n",
        "\n",
        "    # Exeter (university)\n",
        "    exeter_u = pd.DataFrame(index=df_exeter.index)\n",
        "    exeter_u[\"dataset\"] = \"exeter\"\n",
        "    exeter_u[\"entity_type\"] = \"university\"\n",
        "    exeter_u[\"source_id\"] = df_exeter[\"id\"]\n",
        "    exeter_u[\"global_id\"] = \"exeter_\" + df_exeter[\"id\"].astype(str)\n",
        "    exeter_u[\"review_text\"] = df_exeter[\"review_text\"]\n",
        "    exeter_u[\"rating\"] = df_exeter[\"rating\"]\n",
        "    exeter_u[\"rating_bin\"] = df_exeter[\"rating_bin\"]\n",
        "    exeter_u[\"rating_cat\"] = df_exeter[\"rating_cat\"]\n",
        "    exeter_u[\"num_token\"] = df_exeter.get(\"num_token\", 0)\n",
        "    exeter_u[\"length_bin\"] = df_exeter.get(\"length_bin\", 0)\n",
        "    exeter_u[\"sample_criteria\"] = df_exeter.get(\"sample_criteria\", \"unknown\")\n",
        "    exeter_u[\"student_id\"] = df_exeter.get(\"student_id\", \"\")\n",
        "    exeter_u[\"date\"] = df_exeter.get(\"date\", \"\")\n",
        "    exeter_u[\"rating_facilities\"] = df_exeter.get(\"rating_facilities\", 0.0)\n",
        "    exeter_u[\"rating_clubs\"] = df_exeter.get(\"rating_clubs\", 0.0)\n",
        "    exeter_u[\"rating_careerService\"] = df_exeter.get(\"rating_careerService\", 0.0)\n",
        "    exeter_u[\"rating_internet\"] = df_exeter.get(\"rating_internet\", 0.0)\n",
        "    exeter_u = fill_missing_cols(exeter_u, UNIFIED_COLS, DEFAULTS)\n",
        "\n",
        "    df_all = pd.concat([rmp_u, waterloo_u, exeter_u], ignore_index=True)\n",
        "    df_all = df_all[df_all[\"rating_cat\"].isin([\"Neg\", \"Conf/Neu\", \"Pos\"])].reset_index(drop=True)\n",
        "\n",
        "    df_all[\"dataset\"] = df_all[\"dataset\"].astype(\"category\")\n",
        "    df_all[\"entity_type\"] = df_all[\"entity_type\"].astype(\"category\")\n",
        "\n",
        "    print(\"Unified shape:\", df_all.shape)\n",
        "    print(\"\\nDatasets:\")\n",
        "    print(df_all[\"dataset\"].value_counts())\n",
        "    print(\"\\nEntity types:\")\n",
        "    print(df_all[\"entity_type\"].value_counts())\n",
        "    print(\"\\nLabels:\")\n",
        "    print(df_all[\"rating_cat\"].value_counts())\n",
        "\n",
        "    return df_all\n",
        "df_unified = build_unified_df(df_rmp, df_waterloo, df_exeter)\n",
        "unified_path = \"df_unified_raw_clean.csv\"\n",
        "df_unified.to_csv(unified_path, index=False)\n",
        "print(f\"\\nSaved unified dataset to: {unified_path}\")"
      ],
      "metadata": {
        "id": "AIvaVebkJK9s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Unified Dataset Summary Block\n",
        "df_unified[\"useful\"] = df_unified[\"useful\"].fillna(0.0)\n",
        "df_unified[\"easy\"] = df_unified[\"easy\"].fillna(0.0)\n",
        "df_unified[\"student_id\"] = df_unified[\"student_id\"].fillna(\"\")\n",
        "\n",
        "df_all = df_unified.copy()\n",
        "\n",
        "print(\"\\n===== SHAPE =====\")\n",
        "print(df_all.shape)\n",
        "\n",
        "print(\"\\n===== COLUMNS =====\")\n",
        "print(df_all.columns.tolist())\n",
        "\n",
        "print(\"\\n===== DATA TYPES =====\")\n",
        "print(df_all.dtypes)\n",
        "\n",
        "print(\"\\n===== HEAD =====\")\n",
        "display(df_all.head())\n",
        "\n",
        "print(\"\\n===== TAIL =====\")\n",
        "display(df_all.tail())\n",
        "\n",
        "print(\"\\n===== NULL VALUES =====\")\n",
        "print(df_all.isna().sum())\n",
        "\n",
        "print(\"\\n===== BASIC DESCRIPTIVE STATS =====\")\n",
        "display(df_all.describe(include=\"all\"))\n",
        "\n",
        "print(\"\\n===== VALUE COUNTS: dataset =====\")\n",
        "print(df_all[\"dataset\"].value_counts())\n",
        "\n",
        "print(\"\\n===== VALUE COUNTS: entity_type =====\")\n",
        "print(df_all[\"entity_type\"].value_counts())\n",
        "\n",
        "print(\"\\n===== VALUE COUNTS: rating_cat =====\")\n",
        "print(df_all[\"rating_cat\"].value_counts())\n",
        "\n",
        "print(\"\\n===== TEXT LENGTH STATS =====\")\n",
        "df_all[\"review_length\"] = df_all[\"review_text\"].astype(str).apply(len)\n",
        "print(df_all[\"review_length\"].describe())\n",
        "\n",
        "print(\"\\n===== TOKEN COUNT STATS =====\")\n",
        "print(df_all[\"num_token\"].describe())\n",
        "\n",
        "print(\"\\n===== DUPLICATE CHECK =====\")\n",
        "print(\"Duplicates:\", df_all.duplicated().sum())\n",
        "\n",
        "print(\"\\n===== MEMORY USAGE =====\")\n",
        "df_all.info(memory_usage=\"deep\")\n"
      ],
      "metadata": {
        "id": "wIS4_5MrJtPn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Advanced Data Cleaning"
      ],
      "metadata": {
        "id": "0Gk5QFG93Gcy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Strong pre-clean\n",
        "def strong_preclean(text):\n",
        "    if pd.isna(text):\n",
        "        return \"\"\n",
        "    text = str(text)\n",
        "    text = unicodedata.normalize(\"NFKC\", text)\n",
        "\n",
        "    text = re.sub(r\"#NAME\\?\", \" \", text, flags=re.IGNORECASE)\n",
        "    text = emoji.demojize(text, delimiters=(\" \", \" \"))\n",
        "\n",
        "    text = text.replace(\"’\", \"'\").replace(\"`\", \"'\")\n",
        "    text = text.replace(\"'\", \" \")\n",
        "\n",
        "    text = re.sub(r\"([!?]){2,}\", r\"\\1\", text)\n",
        "    text = re.sub(r\"\\.{2,}\", \".\", text)\n",
        "    text = re.sub(r\"[\\r\\n\\t]+\", \" \", text)\n",
        "    text = re.sub(r\"[^A-Za-z0-9\\s\\.\\,\\!\\?\\:\\;\\-]\", \" \", text)\n",
        "\n",
        "    text = re.sub(r\"\\bdon\\s+t\\b\", \"dont\",  text, flags=re.IGNORECASE)\n",
        "    text = re.sub(r\"\\bdidn\\s+t\\b\", \"didnt\", text, flags=re.IGNORECASE)\n",
        "    text = re.sub(r\"\\bcan\\s+t\\b\", \"cant\",  text, flags=re.IGNORECASE)\n",
        "    text = re.sub(r\"\\bwon\\s+t\\b\", \"wont\",  text, flags=re.IGNORECASE)\n",
        "    text = re.sub(r\"\\bisn\\s+t\\b\", \"isnt\",  text, flags=re.IGNORECASE)\n",
        "    text = re.sub(r\"\\baren\\s+t\\b\", \"arent\", text, flags=re.IGNORECASE)\n",
        "    text = re.sub(r\"\\bshouldn\\s+t\\b\", \"shouldnt\", text, flags=re.IGNORECASE)\n",
        "\n",
        "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
        "    return text\n",
        "\n",
        "print(\"Running strong_preclean on review_text...\")\n",
        "df_all[\"review_text_pre\"] = df_all[\"review_text\"].apply(strong_preclean)\n",
        "\n",
        "print(\"\\nSample pre-cleaned rows:\")\n",
        "display(df_all[[\"review_text\", \"review_text_pre\"]].head(5))\n"
      ],
      "metadata": {
        "id": "tFOw-QGX4_fB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Advanced cleaning\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "NEGATIONS = {\n",
        "    \"not\", \"no\", \"never\", \"n't\",\n",
        "    \"dont\", \"didnt\", \"cant\", \"cannot\", \"wont\"\n",
        "}\n",
        "\n",
        "TEACHER_PAT = re.compile(\n",
        "    r\"\\b(prof|professor|dr|mr|mrs|ms)\\.?\\s+[a-z][a-z]+\\b\",\n",
        "    flags=re.IGNORECASE\n",
        ")\n",
        "\n",
        "def get_wordnet_pos(tag: str):\n",
        "    if tag.startswith(\"J\"): return wordnet.ADJ\n",
        "    if tag.startswith(\"V\"): return wordnet.VERB\n",
        "    if tag.startswith(\"N\"): return wordnet.NOUN\n",
        "    if tag.startswith(\"R\"): return wordnet.ADV\n",
        "    return wordnet.NOUN\n",
        "\n",
        "def advanced_clean(text: str) -> str:\n",
        "    if pd.isna(text) or not str(text).strip():\n",
        "        return \"\"\n",
        "\n",
        "    text = str(text).lower()\n",
        "    text = TEACHER_PAT.sub(\" teacher \", text)\n",
        "\n",
        "    tokens = word_tokenize(text)\n",
        "\n",
        "    out_tokens = []\n",
        "    neg_scope = 0\n",
        "    NEG_SCOPE_LEN = 3\n",
        "\n",
        "    for tok in tokens:\n",
        "        if tok in NEGATIONS:\n",
        "            out_tokens.append(tok)\n",
        "            neg_scope = NEG_SCOPE_LEN\n",
        "            continue\n",
        "\n",
        "        if neg_scope > 0 and tok.isalpha():\n",
        "            out_tokens.append(\"neg_\" + tok)\n",
        "            neg_scope -= 1\n",
        "        else:\n",
        "            out_tokens.append(tok)\n",
        "\n",
        "        if tok in [\".\", \"!\", \"?\"]:\n",
        "            neg_scope = 0\n",
        "\n",
        "    pos_tags = pos_tag(out_tokens)\n",
        "\n",
        "    clean_tokens = []\n",
        "    for tok, tag in pos_tags:\n",
        "        prefix = \"\"\n",
        "        base = tok\n",
        "\n",
        "        if tok.startswith(\"neg_\"):\n",
        "            prefix = \"neg_\"\n",
        "            base = tok[4:]\n",
        "\n",
        "        if not base.isalpha():\n",
        "            continue\n",
        "\n",
        "        lemma = lemmatizer.lemmatize(base, pos=get_wordnet_pos(tag))\n",
        "        if len(lemma) <= 1:\n",
        "            continue\n",
        "\n",
        "        clean_tokens.append(prefix + lemma)\n",
        "\n",
        "    return \" \".join(clean_tokens)\n",
        "\n",
        "print(\"Cleaning with advanced_clean...\")\n",
        "df_all[\"text_clean\"] = df_all[\"review_text_pre\"].apply(advanced_clean)\n",
        "df_all[\"clean_len\"] = df_all[\"text_clean\"].str.split().str.len()\n",
        "\n",
        "before = len(df_all)\n",
        "df_all = df_all[df_all[\"text_clean\"].str.strip().str.len() > 0].reset_index(drop=True)\n",
        "after = len(df_all)\n",
        "\n",
        "print(f\"\\nRows before cleaning: {before}\")\n",
        "print(f\"Rows after cleaning:  {after}\")\n",
        "print(\"\\nSample cleaned rows:\")\n",
        "display(df_all[[\"review_text\", \"review_text_pre\", \"text_clean\"]].head(10))\n",
        "print(\"\\nLabel distribution after cleaning:\")\n",
        "print(df_all[\"rating_cat\"].value_counts())\n"
      ],
      "metadata": {
        "id": "X3ErIU50-HnZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openpyxl\n",
        "# Save cleaned dataset\n",
        "df_all.to_csv(\"df_unified_clean_advanced.csv\", index=False)\n",
        "df_all.to_excel(\"df_unified_clean_advanced.xlsx\", index=False)\n",
        "print(\"\\nSaved cleaned dataset to df_unified_clean_advanced.csv and .xlsx\")\n",
        "\n",
        "from google.colab import files\n",
        "files.download(\"df_unified_clean_advanced.csv\")"
      ],
      "metadata": {
        "id": "psL9AurR-SUD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from io import StringIO\n",
        "import sys\n",
        "def full_data_report(df, name=\"DATASET\", save_path=\"full_report.txt\"):\n",
        "    buffer = StringIO()\n",
        "    stdout_original = sys.stdout\n",
        "    sys.stdout = buffer\n",
        "\n",
        "    print(\"=\"*70)\n",
        "    print(f\"FULL DATA REPORT — {name}\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    # 1. Basic Structure\n",
        "    print(\"\\nSHAPE:\")\n",
        "    print(df.shape)\n",
        "\n",
        "    print(\"\\nCOLUMNS:\")\n",
        "    print(df.columns.tolist())\n",
        "\n",
        "    print(\"\\nDATA TYPES:\")\n",
        "    print(df.dtypes)\n",
        "\n",
        "    print(\"\\nMEMORY USAGE (deep):\")\n",
        "    print(df.memory_usage(deep=True))\n",
        "\n",
        "    # 2. Preview\n",
        "    print(\"\\nHEAD:\")\n",
        "    print(df.head(5))\n",
        "\n",
        "    print(\"\\nTAIL:\")\n",
        "    print(df.tail(5))\n",
        "\n",
        "    # 3. Summary Stats\n",
        "    num_cols = df.select_dtypes(include=[\"int64\", \"float64\"]).columns\n",
        "    print(\"\\nDESCRIPTIVE STATS — NUMERIC:\")\n",
        "    print(df[num_cols].describe().T if len(num_cols) else \"No numeric columns.\")\n",
        "\n",
        "    cat_cols = df.select_dtypes(include=\"object\").columns\n",
        "    print(\"\\nDESCRIPTIVE STATS — CATEGORICAL:\")\n",
        "    print(df[cat_cols].describe().T if len(cat_cols) else \"No object columns.\")\n",
        "\n",
        "    # 4. Missing Values\n",
        "    print(\"\\nMISSING VALUES:\")\n",
        "    missing = df.isna().sum()\n",
        "    print(missing[missing > 0] if missing.sum() > 0 else \"No missing values detected.\")\n",
        "\n",
        "    print(\"\\nEMPTY STRINGS:\")\n",
        "    empty = (df.select_dtypes(include=\"object\") == \"\").sum()\n",
        "    empty = empty[empty > 0]\n",
        "    print(empty if len(empty) else \"No empty strings detected.\")\n",
        "\n",
        "    # 5. Duplicates\n",
        "    print(\"\\nDUPLICATE ROWS:\")\n",
        "    print(df.duplicated().sum())\n",
        "    if \"global_id\" in df.columns:\n",
        "        print(\"Duplicate global_id:\", df[\"global_id\"].duplicated().sum())\n",
        "\n",
        "    # 6. Key Value Counts\n",
        "    inspect_cols = [\"dataset\", \"entity_type\", \"rating_cat\"]\n",
        "    print(\"\\nKEY VALUE DISTRIBUTIONS:\")\n",
        "    for col in inspect_cols:\n",
        "        if col in df.columns:\n",
        "            print(f\"\\n{col}:\")\n",
        "            print(df[col].value_counts())\n",
        "\n",
        "    # 7. Text Analysis\n",
        "    if \"review_text\" in df.columns:\n",
        "        print(\"\\nREVIEW LENGTH STATS:\")\n",
        "        print(df[\"review_text\"].str.len().describe())\n",
        "\n",
        "    if \"text_clean\" in df.columns:\n",
        "        print(\"\\nCLEANED TEXT LENGTH STATS:\")\n",
        "        print(df[\"text_clean\"].str.split().str.len().describe())\n",
        "\n",
        "    # 8. Non-ASCII Check\n",
        "    print(\"\\nNON-ASCII CHARACTER COUNTS:\")\n",
        "    non_ascii = df.apply(lambda col: col.apply(\n",
        "        lambda x: any(ord(c) > 127 for c in str(x))) if col.dtype == \"object\" else None\n",
        "    )\n",
        "    non_ascii_counts = non_ascii.sum()\n",
        "    non_ascii_counts = non_ascii_counts[non_ascii_counts > 0]\n",
        "    print(non_ascii_counts if len(non_ascii_counts) else \"No non-ASCII characters found.\")\n",
        "\n",
        "    # 9. Quantiles\n",
        "    if len(num_cols) > 0:\n",
        "        print(\"\\nQUANTILES (0.01 to 0.99):\")\n",
        "        print(df[num_cols].quantile([0.01, 0.25, 0.5, 0.75, 0.99]))\n",
        "\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"REPORT COMPLETE\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    # Restore print output to notebook\n",
        "    sys.stdout = stdout_original\n",
        "    report_text = buffer.getvalue()\n",
        "    with open(save_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        f.write(report_text)\n",
        "    print(report_text)\n",
        "\n",
        "    print(f\"\\nReport saved to: {save_path}\")\n",
        "full_data_report(df_all, name=\"Unified Cleaned Dataset\", save_path=\"Unified_Cleaned_Report.txt\")"
      ],
      "metadata": {
        "id": "1042KyxtCa_X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Rename final cleaned DataFrame\n",
        "final_cleaned_unified = df_all.copy()\n",
        "\n",
        "print(\"Final cleaned dataset ready:\")\n",
        "print(\"Variable name: final_cleaned_unified\")\n",
        "print(\"Shape:\", final_cleaned_unified.shape)\n",
        "print(\"Label counts:\")\n",
        "print(final_cleaned_unified[\"rating_cat\"].value_counts())"
      ],
      "metadata": {
        "id": "IeKLg3xSR4Su"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Visual** **EDA**"
      ],
      "metadata": {
        "id": "XUhnn2FqVEow"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from wordcloud import WordCloud\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from sklearn.manifold import TSNE\n",
        "from nltk import word_tokenize, pos_tag\n",
        "df = final_cleaned_unified.copy()\n",
        "sns.set(style=\"whitegrid\")\n",
        "\n"
      ],
      "metadata": {
        "id": "IsSYdOetp57F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Basic structure + label / dataset / entity distributions"
      ],
      "metadata": {
        "id": "BQm-o81oqf8Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Label distribution\n",
        "plt.figure(figsize=(6,4))\n",
        "sns.countplot(data=df, x=\"rating_cat\", order=[\"Neg\",\"Conf/Neu\",\"Pos\"])\n",
        "plt.title(\"Rating Category Distribution\")\n",
        "plt.xlabel(\"Label\")\n",
        "plt.ylabel(\"Count\")\n",
        "plt.show()\n",
        "\n",
        "# Dataset × label\n",
        "plt.figure(figsize=(8,5))\n",
        "sns.countplot(data=df, x=\"dataset\", hue=\"rating_cat\",\n",
        "              hue_order=[\"Neg\",\"Conf/Neu\",\"Pos\"])\n",
        "plt.title(\"Sentiment Distribution per Dataset\")\n",
        "plt.xlabel(\"Dataset\")\n",
        "plt.ylabel(\"Count\")\n",
        "plt.legend(title=\"Label\")\n",
        "plt.show()\n",
        "\n",
        "# Entity type × label\n",
        "plt.figure(figsize=(8,5))\n",
        "sns.countplot(data=df, x=\"entity_type\", hue=\"rating_cat\",\n",
        "              hue_order=[\"Neg\",\"Conf/Neu\",\"Pos\"])\n",
        "plt.title(\"Sentiment Distribution per Entity Type\")\n",
        "plt.xlabel(\"Entity Type\")\n",
        "plt.ylabel(\"Count\")\n",
        "plt.legend(title=\"Label\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "gTKJYVbdqaXQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Text length"
      ],
      "metadata": {
        "id": "tPz53Yb1qpRg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Histogram of cleaned length\n",
        "plt.figure(figsize=(8,4))\n",
        "sns.histplot(df[\"clean_len\"], bins=60, kde=True)\n",
        "plt.title(\"Distribution of Cleaned Text Length (Tokens)\")\n",
        "plt.xlabel(\"Token Count\")\n",
        "plt.ylabel(\"Frequency\")\n",
        "plt.show()\n",
        "\n",
        "# Boxplot by dataset\n",
        "plt.figure(figsize=(8,5))\n",
        "sns.boxplot(data=df, x=\"dataset\", y=\"clean_len\")\n",
        "plt.title(\"Cleaned Review Length Across Datasets\")\n",
        "plt.xlabel(\"Dataset\")\n",
        "plt.ylabel(\"Token Count\")\n",
        "plt.show()\n",
        "\n",
        "# Boxplot by label\n",
        "plt.figure(figsize=(6,4))\n",
        "sns.boxplot(data=df, x=\"rating_cat\", y=\"clean_len\",\n",
        "            order=[\"Neg\",\"Conf/Neu\",\"Pos\"])\n",
        "plt.title(\"Text Length by Sentiment Label\")\n",
        "plt.xlabel(\"Label\")\n",
        "plt.ylabel(\"Cleaned Token Count\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "knMth3khqnG7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Correlation heatmap of numeric features"
      ],
      "metadata": {
        "id": "oTo5p3LVqvcq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_cols = [\n",
        "    \"rating\",\"num_token\",\"num_ratings\",\"num_reviews\",\n",
        "    \"useful\",\"easy\",\"liked\",\n",
        "    \"rating_facilities\",\"rating_clubs\",\"rating_careerService\",\n",
        "    \"rating_internet\",\"clean_len\"\n",
        "]\n",
        "corr = df[num_cols].corr()\n",
        "plt.figure(figsize=(10,9), dpi=120)\n",
        "sns.heatmap(\n",
        "    corr, annot=True, fmt=\".2f\",\n",
        "    cmap=\"coolwarm\", vmin=-1, vmax=1,\n",
        "    linewidths=0.5, cbar_kws={'shrink':0.8}\n",
        ")\n",
        "plt.title(\"Correlation Heatmap of Numeric Features\", fontsize=16)\n",
        "plt.xticks(rotation=45, ha=\"right\")\n",
        "plt.yticks(rotation=0)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "dEdlJMp8qnES"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. WordClouds (global + by label + by entity)"
      ],
      "metadata": {
        "id": "GlTmD42Vq014"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Global WC\n",
        "all_text = \" \".join(df[\"text_clean\"].tolist())\n",
        "wc = WordCloud(width=1400, height=800,\n",
        "               background_color=\"white\",\n",
        "               max_words=300, collocations=False).generate(all_text)\n",
        "\n",
        "plt.figure(figsize=(12,8))\n",
        "plt.imshow(wc, interpolation=\"bilinear\")\n",
        "plt.axis(\"off\")\n",
        "plt.title(\"WordCloud — All Cleaned Reviews\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "rjY9Z_ssqnBM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Label-wise WC\n",
        "for lab in [\"Pos\", \"Neg\", \"Conf/Neu\"]:\n",
        "    subset_text = \" \".join(df[df[\"rating_cat\"] == lab][\"text_clean\"])\n",
        "    wc = WordCloud(width=1600, height=900,\n",
        "                   background_color=\"white\",\n",
        "                   max_words=200, collocations=False).generate(subset_text)\n",
        "    plt.figure(figsize=(14,8))\n",
        "    plt.imshow(wc, interpolation=\"bilinear\")\n",
        "    plt.axis(\"off\")\n",
        "    plt.title(f\"WordCloud — {lab} Reviews\")\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "uWIH1YEPqm-Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Entity-wise WC\n",
        "for ent in [\"teacher\", \"course\", \"university\"]:\n",
        "    subset_text = \" \".join(df[df[\"entity_type\"] == ent][\"text_clean\"])\n",
        "    wc = WordCloud(width=1600, height=900,\n",
        "                   background_color=\"white\",\n",
        "                   max_words=200, collocations=False).generate(subset_text)\n",
        "    plt.figure(figsize=(14,8))\n",
        "    plt.imshow(wc, interpolation=\"bilinear\")\n",
        "    plt.axis(\"off\")\n",
        "    plt.title(f\"WordCloud — {ent.capitalize()} Reviews\")\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "0tHbSKt3qm7p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. N-grams (bigrams/trigrams) — helper + plots"
      ],
      "metadata": {
        "id": "FAazVehfrBSI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "def get_top_ngrams(texts, ngram_range=(2,2), top_n=20, min_df=5):\n",
        "    vec = CountVectorizer(ngram_range=ngram_range, min_df=min_df)\n",
        "    X = vec.fit_transform(texts)\n",
        "    counts = X.sum(axis=0).A1\n",
        "    vocab = vec.get_feature_names_out()\n",
        "    df_ng = pd.DataFrame({\"ngram\": vocab, \"count\": counts})\n",
        "    return df_ng.sort_values(\"count\", ascending=False).head(top_n)\n",
        "\n",
        "def plot_ngrams(df_ng, title):\n",
        "    plt.figure(figsize=(10,6))\n",
        "    sns.barplot(data=df_ng, x=\"count\", y=\"ngram\", color=\"#F0D98C\")\n",
        "    plt.title(title)\n",
        "    plt.xlabel(\"Frequency\")\n",
        "    plt.ylabel(\"N-gram\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "umYMhR0Nqm5F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Label-wise bigrams & trigrams"
      ],
      "metadata": {
        "id": "Ehf0IKXTrQ1F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for lab in [\"Pos\", \"Neg\", \"Conf/Neu\"]:\n",
        "    sub = df[df[\"rating_cat\"] == lab][\"text_clean\"]\n",
        "    bi = get_top_ngrams(sub, (2,2), 20)\n",
        "    tri = get_top_ngrams(sub, (3,3), 20)\n",
        "\n",
        "    plot_ngrams(bi,  f\"Top 20 Bigrams — {lab}\")\n",
        "    plot_ngrams(tri, f\"Top 20 Trigrams — {lab}\")\n"
      ],
      "metadata": {
        "id": "iYs-aiBVqm2a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6 Lexical richness & complexity"
      ],
      "metadata": {
        "id": "9H1weXjJsqYI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def lexical_stats(group_df):\n",
        "    tokens = group_df[\"text_clean\"].str.split()\n",
        "    n_tokens = tokens.apply(len).sum()\n",
        "    vocab = set(w for row in tokens for w in row)\n",
        "    n_types = len(vocab)\n",
        "    ttr = n_types / n_tokens if n_tokens > 0 else 0\n",
        "    return pd.Series({\n",
        "        \"n_docs\": len(group_df),\n",
        "        \"tokens\": n_tokens,\n",
        "        \"types\": n_types,\n",
        "        \"TTR\": ttr,\n",
        "        \"avg_len\": group_df[\"clean_len\"].mean()\n",
        "    })\n"
      ],
      "metadata": {
        "id": "s2Bt09FKrTjn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# By label\n",
        "lex_by_label = df.groupby(\"rating_cat\").apply(lexical_stats)\n",
        "print(\"\\nLexical stats by label:\")\n",
        "display(lex_by_label)\n",
        "\n",
        "# By dataset\n",
        "lex_by_dataset = df.groupby(\"dataset\").apply(lexical_stats)\n",
        "print(\"\\nLexical stats by dataset:\")\n",
        "display(lex_by_dataset)\n"
      ],
      "metadata": {
        "id": "OzOl7I8frThW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##7. POS tag distribution (per label)"
      ],
      "metadata": {
        "id": "DgLcadIFszTm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_pos_counts(text, max_tokens=100):\n",
        "    tokens = word_tokenize(text)[:max_tokens]\n",
        "    tags = [tag for _, tag in pos_tag(tokens)]\n",
        "    return pd.Series(tags).value_counts()\n",
        "\n",
        "def pos_distribution(df_subset, sample_size=2000):\n",
        "    # sample to keep it fast\n",
        "    sample = df_subset.sample(min(len(df_subset), sample_size), random_state=42)\n",
        "    pos_counts = sample[\"text_clean\"].apply(get_pos_counts).fillna(0)\n",
        "    return pos_counts.sum().sort_values(ascending=False)\n",
        "pos_by_label = {}\n",
        "for lab in [\"Pos\", \"Neg\", \"Conf/Neu\"]:\n",
        "    pos_by_label[lab] = pos_distribution(df[df[\"rating_cat\"] == lab])\n",
        "\n",
        "pos_df = pd.DataFrame(pos_by_label).fillna(0)\n",
        "print(pos_df.head(15))  # top POS tags\n",
        "# Visualise a few key tags (e.g., adjectives JJ, verbs VB/VBD/VBZ, adverbs RB)\n",
        "key_tags = [\"JJ\",\"JJR\",\"JJS\",\"VB\",\"VBD\",\"VBG\",\"VBN\",\"VBP\",\"VBZ\",\"RB\",\"RBR\",\"RBS\"]\n",
        "subset = pos_df.loc[key_tags].T  # rows=labels, cols=tags\n",
        "\n",
        "subset.plot(kind=\"bar\", figsize=(10,5))\n",
        "plt.title(\"POS Distribution (selected tags) by Label\")\n",
        "plt.xlabel(\"Label\")\n",
        "plt.ylabel(\"Count (sampled)\")\n",
        "plt.xticks(rotation=0)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "dC5Xaoqys0ZY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 8. Embedding-based visualisation (TF-IDF + SVD + t-SNE)"
      ],
      "metadata": {
        "id": "t-FgZaArtDRy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sample = df.sample(4000, random_state=42)\n",
        "texts = sample[\"text_clean\"].tolist()\n",
        "labels = sample[\"rating_cat\"].tolist()\n",
        "\n",
        "tfidf = TfidfVectorizer(max_features=5000, min_df=5)\n",
        "X = tfidf.fit_transform(texts)\n",
        "\n",
        "svd = TruncatedSVD(n_components=50, random_state=42)\n",
        "X_reduced = svd.fit_transform(X)\n",
        "\n",
        "tsne = TSNE(n_components=2, perplexity=40, random_state=42, n_iter=2000)\n",
        "X_tsne = tsne.fit_transform(X_reduced)\n",
        "\n",
        "sample[\"tsne_x\"] = X_tsne[:,0]\n",
        "sample[\"tsne_y\"] = X_tsne[:,1]\n",
        "\n",
        "plt.figure(figsize=(8,6))\n",
        "sns.scatterplot(data=sample, x=\"tsne_x\", y=\"tsne_y\",\n",
        "                hue=\"rating_cat\", hue_order=[\"Neg\",\"Conf/Neu\",\"Pos\"],\n",
        "                alpha=0.6, s=20)\n",
        "plt.title(\"t-SNE of Review Representations (TF-IDF)\")\n",
        "plt.xlabel(\"\")\n",
        "plt.ylabel(\"\")\n",
        "plt.legend(title=\"Label\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "lwvFHsb5s0Tr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 9. Outlier check (very long reviews)"
      ],
      "metadata": {
        "id": "Faad3BXGtOJz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "q99 = df[\"clean_len\"].quantile(0.99)\n",
        "print(\"99th percentile of clean_len:\", q99)\n",
        "\n",
        "outliers = df[df[\"clean_len\"] > q99]\n",
        "print(\"Number of outlier reviews:\", len(outliers))\n",
        "\n",
        "# Quick peek\n",
        "display(outliers[[\"dataset\",\"entity_type\",\"rating_cat\",\"clean_len\",\"review_text\"]].head())\n"
      ],
      "metadata": {
        "id": "tsYIAESZtL5C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.countplot(data=final_cleaned_unified, x=\"dataset\", hue=\"rating_cat\")\n",
        "plt.title(\"Sentiment Distribution per Dataset\")\n",
        "plt.xlabel(\"Dataset\")\n",
        "plt.ylabel(\"Count\")\n",
        "plt.legend(title=\"Label\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "YpOHG3izYCsA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 10. Topic Modelling using LDA (BoW)"
      ],
      "metadata": {
        "id": "jzMh9Tw_YVfz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "from nltk.corpus import stopwords\n",
        "import nltk\n",
        "nltk.download(\"stopwords\")\n",
        "\n",
        "# Stopwords\n",
        "stop_words = set(stopwords.words(\"english\"))\n",
        "\n",
        "# Add domain-specific stopwords\n",
        "domain_stopwords = {\n",
        "    \"class\", \"course\", \"teacher\", \"professor\", \"lecture\",\n",
        "    \"exam\", \"assignment\", \"student\", \"work\", \"quiz\",\n",
        "    \"midterm\", \"final\", \"mark\", \"grade\"\n",
        "}\n",
        "\n",
        "stop_words = stop_words.union(domain_stopwords)\n",
        "\n",
        "# Convert set → list for CountVectorizer\n",
        "stop_words = list(stop_words)\n",
        "\n",
        "# Vectorizer\n",
        "vectorizer = CountVectorizer(\n",
        "    max_df=0.90,\n",
        "    min_df=50,\n",
        "    stop_words=stop_words\n",
        ")\n",
        "\n",
        "X = vectorizer.fit_transform(final_cleaned_unified[\"text_clean\"])\n",
        "\n",
        "# LDA Model\n",
        "lda = LatentDirichletAllocation(\n",
        "    n_components=6,\n",
        "    learning_method=\"batch\",\n",
        "    random_state=42\n",
        ")\n",
        "lda.fit(X)\n",
        "\n",
        "# Display topics\n",
        "terms = vectorizer.get_feature_names_out()\n",
        "\n",
        "for idx, topic in enumerate(lda.components_):\n",
        "    print(f\"\\n TOPIC {idx+1}\")\n",
        "    print([terms[i] for i in topic.argsort()[-15:]])\n"
      ],
      "metadata": {
        "id": "4I7E7SpyYWCc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 11. Keyword Importance per Emotion (Chi-Square)"
      ],
      "metadata": {
        "id": "Gpo0rVS7Yc4C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_selection import chi2\n",
        "vec = CountVectorizer(min_df=20)\n",
        "X = vec.fit_transform(final_cleaned_unified[\"text_clean\"])\n",
        "y = final_cleaned_unified[\"rating_cat\"]\n",
        "\n",
        "chi2_scores, p = chi2(X, y)\n",
        "scores = pd.DataFrame({\"term\": vec.get_feature_names_out(), \"score\": chi2_scores})\n",
        "top_scores = scores.sort_values(\"score\", ascending=False)\n",
        "\n",
        "print(top_scores.head(30))\n"
      ],
      "metadata": {
        "id": "70S6r8HTYaDu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 12. Emotion Trend by Review Length"
      ],
      "metadata": {
        "id": "5KgZCoZmYk_s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sns.boxplot(data=final_cleaned_unified, x=\"rating_cat\", y=\"clean_len\")\n",
        "plt.title(\"Text Length by Sentiment Label\")\n",
        "plt.xlabel(\"Label\")\n",
        "plt.ylabel(\"Cleaned Token Count\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "PERIUr6TYftA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "NT2YNJUKSjR9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Data Preprocessing**"
      ],
      "metadata": {
        "id": "aBHPTUtqSnsO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Feature engineering on final_cleaned_unified → df_fe"
      ],
      "metadata": {
        "id": "RrkoJCyDcr9Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_fe = final_cleaned_unified.copy().reset_index(drop=True)\n",
        "\n",
        "label_order = [\"Neg\", \"Conf/Neu\", \"Pos\"]\n",
        "label2id = {lab: i for i, lab in enumerate(label_order)}\n",
        "id2label = {i: lab for lab, i in label2id.items()}\n",
        "\n",
        "df_fe = df_fe[df_fe[\"rating_cat\"].isin(label_order)].reset_index(drop=True)\n",
        "df_fe[\"label_id\"] = df_fe[\"rating_cat\"].map(label2id).astype(int)\n",
        "\n",
        "df_fe[\"clean_len\"] = df_fe[\"text_clean\"].str.split().str.len()\n",
        "\n",
        "def neg_ratio(text):\n",
        "    toks = str(text).split()\n",
        "    if not toks:\n",
        "        return 0.0\n",
        "    return sum(1 for t in toks if t.startswith(\"neg_\")) / len(toks)\n",
        "\n",
        "df_fe[\"neg_ratio\"] = df_fe[\"text_clean\"].apply(neg_ratio)\n",
        "df_fe[\"exclam_count\"] = df_fe[\"review_text_pre\"].str.count(\"!\")\n",
        "df_fe[\"quest_count\"]  = df_fe[\"review_text_pre\"].str.count(r\"\\?\")\n",
        "\n",
        "df_fe = pd.get_dummies(\n",
        "    df_fe,\n",
        "    columns=[\"dataset\", \"entity_type\"],\n",
        "    drop_first=True\n",
        ")\n",
        "\n",
        "extra_feat_cols = [\n",
        "    \"clean_len\",\n",
        "    \"neg_ratio\",\n",
        "    \"exclam_count\",\n",
        "    \"quest_count\",\n",
        "    \"dataset_rmp\",\n",
        "    \"dataset_waterloo\",\n",
        "    \"entity_type_teacher\",\n",
        "    \"entity_type_university\",\n",
        "]\n",
        "extra_feat_cols = [c for c in extra_feat_cols if c in df_fe.columns]\n",
        "\n",
        "print(\"Feature-engineering base shape:\", df_fe.shape)\n",
        "print(\"Extra numeric features:\", extra_feat_cols)\n",
        "print(\"Label mapping:\", label2id)\n",
        "print(df_fe[\"rating_cat\"].value_counts())\n",
        "df_fe[extra_feat_cols].describe()"
      ],
      "metadata": {
        "id": "kk0dHKTjSm-4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Stratified train/val/test split (70/15/15) + class weights from train"
      ],
      "metadata": {
        "id": "a2oPDbMPc2mz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_all = df_fe[\"label_id\"].values\n",
        "indices = np.arange(len(df_fe))\n",
        "\n",
        "train_idx, temp_idx, y_train, y_temp = train_test_split(\n",
        "    indices,\n",
        "    y_all,\n",
        "    test_size=0.30,\n",
        "    stratify=y_all,\n",
        "    random_state=RANDOM_SEED,\n",
        ")\n",
        "\n",
        "val_idx, test_idx, y_val, y_test = train_test_split(\n",
        "    temp_idx,\n",
        "    y_temp,\n",
        "    test_size=0.50,\n",
        "    stratify=y_temp,\n",
        "    random_state=RANDOM_SEED,\n",
        ")\n",
        "\n",
        "print(\"Train size:\", len(train_idx))\n",
        "print(\"Val size  :\", len(val_idx))\n",
        "print(\"Test size :\", len(test_idx))\n",
        "\n",
        "classes = np.unique(y_train)\n",
        "weights = compute_class_weight(\n",
        "    class_weight=\"balanced\",\n",
        "    classes=classes,\n",
        "    y=y_train,\n",
        ")\n",
        "class_weight_dict = {int(c): float(w) for c, w in zip(classes, weights)}\n",
        "\n",
        "print(\"Class weights (train only):\", class_weight_dict)\n",
        "print(\"id2label:\", id2label)\n"
      ],
      "metadata": {
        "id": "c-DXW5Zfc0E7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## TF-IDF"
      ],
      "metadata": {
        "id": "SzX2Djdjc-XX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_text = df_fe.loc[train_idx, \"text_clean\"].astype(str)\n",
        "X_val_text   = df_fe.loc[val_idx,   \"text_clean\"].astype(str)\n",
        "X_test_text  = df_fe.loc[test_idx,  \"text_clean\"].astype(str)\n",
        "\n",
        "tfidf_vectorizer = TfidfVectorizer(\n",
        "    max_features=40000,\n",
        "    ngram_range=(1, 2),\n",
        "    min_df=5,\n",
        "    max_df=0.9,\n",
        ")\n",
        "\n",
        "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train_text)\n",
        "X_val_tfidf   = tfidf_vectorizer.transform(X_val_text)\n",
        "X_test_tfidf  = tfidf_vectorizer.transform(X_test_text)\n",
        "\n",
        "print(\"TF-IDF shapes:\")\n",
        "print(\"  Train:\", X_train_tfidf.shape)\n",
        "print(\"  Val  :\", X_val_tfidf.shape)\n",
        "print(\"  Test :\", X_test_tfidf.shape)\n"
      ],
      "metadata": {
        "id": "XEvgDHj5c5pP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Scale numeric features (train only) + combine with TF-IDF"
      ],
      "metadata": {
        "id": "qgz0W56ndtvJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_meta_train = df_fe.loc[train_idx, extra_feat_cols].astype(float).values\n",
        "X_meta_val   = df_fe.loc[val_idx,   extra_feat_cols].astype(float).values\n",
        "X_meta_test  = df_fe.loc[test_idx,  extra_feat_cols].astype(float).values\n",
        "\n",
        "X_meta_train = np.nan_to_num(X_meta_train, nan=0.0)\n",
        "X_meta_val   = np.nan_to_num(X_meta_val,   nan=0.0)\n",
        "X_meta_test  = np.nan_to_num(X_meta_test,  nan=0.0)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_meta_train_scaled = scaler.fit_transform(X_meta_train)\n",
        "X_meta_val_scaled   = scaler.transform(X_meta_val)\n",
        "X_meta_test_scaled  = scaler.transform(X_meta_test)\n",
        "\n",
        "X_meta_train_sp = csr_matrix(X_meta_train_scaled)\n",
        "X_meta_val_sp   = csr_matrix(X_meta_val_scaled)\n",
        "X_meta_test_sp  = csr_matrix(X_meta_test_scaled)\n",
        "\n",
        "X_train_combined = hstack([X_train_tfidf, X_meta_train_sp])\n",
        "X_val_combined   = hstack([X_val_tfidf,   X_meta_val_sp])\n",
        "X_test_combined  = hstack([X_test_tfidf,  X_meta_test_sp])\n",
        "\n",
        "print(\"Combined feature shapes:\")\n",
        "print(\"  Train:\", X_train_combined.shape)\n",
        "print(\"  Val  :\", X_val_combined.shape)\n",
        "print(\"  Test :\", X_test_combined.shape)\n"
      ],
      "metadata": {
        "id": "QRZRKRv6dB3w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Baseline Models**"
      ],
      "metadata": {
        "id": "E9PEKr_19UEK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##LOGISTIC REGRESSION"
      ],
      "metadata": {
        "id": "eCuZ5zhWdzio"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import (\n",
        "    classification_report, confusion_matrix, ConfusionMatrixDisplay,\n",
        "    roc_curve, auc, roc_auc_score\n",
        ")\n",
        "\n",
        "labels = [\"Neg\", \"Conf/Neu\", \"Pos\"]\n",
        "\n",
        "# 1) GridSearchCV on Logistic Regression\n",
        "param_grid = {\"C\": [0.5, 1.0, 2.0]}\n",
        "\n",
        "log_reg = LogisticRegression(\n",
        "    max_iter=2000,\n",
        "    class_weight=\"balanced\",\n",
        "    n_jobs=-1,\n",
        "    solver=\"lbfgs\"\n",
        ")\n",
        "\n",
        "grid = GridSearchCV(\n",
        "    estimator=log_reg,\n",
        "    param_grid=param_grid,\n",
        "    scoring=\"f1_macro\",\n",
        "    cv=5,\n",
        "    n_jobs=-1,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "grid.fit(X_train_combined, y_train)\n",
        "best_lr = grid.best_estimator_\n",
        "\n",
        "print(\"\\nBest Hyperparameters:\", grid.best_params_)\n",
        "\n",
        "# 2) Validation performance\n",
        "y_val_pred = best_lr.predict(X_val_combined)\n",
        "print(\"\\n=== Logistic Regression (Validation) ===\")\n",
        "print(classification_report(y_val, y_val_pred, target_names=labels))\n",
        "\n",
        "# 3) Test performance\n",
        "y_test_pred = best_lr.predict(X_test_combined)\n",
        "print(\"\\n=== Logistic Regression (Test) ===\")\n",
        "print(classification_report(y_test, y_test_pred, target_names=labels))\n",
        "\n",
        "print(\"\\nConfusion Matrix (Test):\")\n",
        "cm_test = confusion_matrix(y_test, y_test_pred)\n",
        "print(cm_test)\n",
        "\n",
        "# 4) THRESHOLD TUNING for Conf/Neu (post-hoc)\n",
        "val_probs = best_lr.predict_proba(X_val_combined)\n",
        "test_probs = best_lr.predict_proba(X_test_combined)\n",
        "\n",
        "def predict_with_alpha_probs(probs, alpha_conf):\n",
        "    \"\"\"Boost Conf/Neu probability by alpha_conf and renormalise.\"\"\"\n",
        "    p = probs.copy()\n",
        "    p[:, 1] = p[:, 1] * alpha_conf\n",
        "    p = p / p.sum(axis=1, keepdims=True)\n",
        "    return np.argmax(p, axis=1)\n",
        "\n",
        "alphas = [1.0, 1.2, 1.4, 1.6, 2.0]\n",
        "best_macro, best_alpha = -1, 1.0\n",
        "\n",
        "for a in alphas:\n",
        "    y_val_adj = predict_with_alpha_probs(val_probs, a)\n",
        "    y_val_onehot_pred = np.eye(3)[y_val_adj]\n",
        "    macro = roc_auc_score(y_val, y_val_onehot_pred, multi_class=\"ovr\", average=\"macro\")\n",
        "    print(f\"alpha={a:.1f} → macro score (approx): {macro:.4f}\")\n",
        "    if macro > best_macro:\n",
        "        best_macro = macro\n",
        "        best_alpha = a\n",
        "\n",
        "print(\"\\nBest alpha:\", best_alpha, \" | Best macro (approx):\", best_macro)\n",
        "\n",
        "y_val_final = predict_with_alpha_probs(val_probs, best_alpha)\n",
        "y_test_final = predict_with_alpha_probs(test_probs, best_alpha)\n",
        "\n",
        "print(\"\\n=== Logistic Regression (Validation, tuned threshold) ===\")\n",
        "print(classification_report(y_val, y_val_final, target_names=labels))\n",
        "\n",
        "print(\"\\n=== Logistic Regression (Test, tuned threshold) ===\")\n",
        "print(classification_report(y_test, y_test_final, target_names=labels))\n",
        "\n",
        "# 5) Plot Confusion Matrix (Test, tuned)\n",
        "plt.figure(figsize=(6, 5))\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix(y_test, y_test_final), display_labels=labels)\n",
        "disp.plot(cmap=\"Blues\", values_format=\"d\")\n",
        "plt.title(\"Logistic Regression - Confusion Matrix (Test, tuned)\")\n",
        "plt.show()\n",
        "\n",
        "# 6) ROC curves (multiclass, one-vs-rest, untuned probs)\n",
        "y_test_bin = np.eye(3)[y_test]\n",
        "y_test_prob = best_lr.predict_proba(X_test_combined)\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "for i in range(3):\n",
        "    fpr, tpr, _ = roc_curve(y_test_bin[:, i], y_test_prob[:, i])\n",
        "    roc_auc = auc(fpr, tpr)\n",
        "    plt.plot(fpr, tpr, label=f\"{labels[i]} (AUC={roc_auc:.3f})\")\n",
        "\n",
        "plt.plot([0, 1], [0, 1], \"k--\")\n",
        "plt.xlabel(\"False Positive Rate\")\n",
        "plt.ylabel(\"True Positive Rate\")\n",
        "plt.title(\"Logistic Regression - ROC (Test)\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "macro_auc = roc_auc_score(y_test_bin, y_test_prob, average=\"macro\")\n",
        "print(\"\\nMacro ROC-AUC (Test):\", macro_auc)\n"
      ],
      "metadata": {
        "id": "qNKujtVUplTJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.sparse import hstack, csr_matrix\n",
        "example_texts = [\n",
        "    \"The lectures are poorly structured, I rarely understand the topics and the feedback is always unclear. Overall, the module is frustrating and overwhelming.\",\n",
        "    \"The experience with the module is neutral. It does not feel positive or negative, and the overall impression remains evenly balanced without any strong sentiment.\",\n",
        "    \"The lecturer explains everything clearly, the examples are helpful, and the content is extremely engaging. I genuinely enjoy attending this module every week.\"\n",
        "]\n",
        "\n",
        "def basic_clean(text):\n",
        "    return text.lower().strip()\n",
        "\n",
        "example_clean = [basic_clean(t) for t in example_texts]\n",
        "\n",
        "# 1) TF-IDF features for the examples\n",
        "X_example_tfidf = tfidf_vectorizer.transform(example_clean)\n",
        "\n",
        "# 2) Match the feature size of X_train_combined used to train best_lr\n",
        "n_features_model = best_lr.n_features_in_\n",
        "n_features_tfidf = X_example_tfidf.shape[1]\n",
        "n_extra = n_features_model - n_features_tfidf\n",
        "\n",
        "if n_extra < 0:\n",
        "    raise ValueError(\n",
        "        f\"TF-IDF has more features ({n_features_tfidf}) than model was trained on ({n_features_model}).\"\n",
        "    )\n",
        "\n",
        "# Create dummy extra features (zeros) for the examples\n",
        "extra_example = csr_matrix(np.zeros((X_example_tfidf.shape[0], n_extra)))\n",
        "\n",
        "# Combine TF-IDF + extra to match X_train_combined shape\n",
        "X_example_combined = hstack([X_example_tfidf, extra_example])\n",
        "\n",
        "# 3) Predict with tuned thresholds\n",
        "example_probs = best_lr.predict_proba(X_example_combined)\n",
        "example_pred_idx = predict_with_alpha_probs(example_probs, best_alpha)\n",
        "\n",
        "print(\"\\n=== REAL-TIME PREDICTIONS (Logistic Regression) ===\")\n",
        "for text, idx, prob in zip(example_texts, example_pred_idx, example_probs):\n",
        "    print(\"\\nText:\", text)\n",
        "    print(\"Predicted label:\", labels[idx])\n",
        "    print(\"Class probabilities [Neg, Conf/Neu, Pos]:\", np.round(prob, 3))\n"
      ],
      "metadata": {
        "id": "AOeh-ZI8tRMD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# REAL-TIME PREDICTIONS + RISK (Logistic Regression)\n",
        "# ==========================================\n",
        "import numpy as np\n",
        "from scipy.sparse import hstack, csr_matrix\n",
        "\n",
        "label_names = [\"Neg\", \"Conf/Neu\", \"Pos\"]\n",
        "\n",
        "example_texts = [\n",
        "    \"The lectures are poorly structured and I rarely understand the topics. Overall, this module feels frustrating.\",\n",
        "    \"Some weeks are clear, some are confusing. Overall it's okay but I am not fully confident.\",\n",
        "    \"The lecturer is amazing, explains everything clearly and the module is very engaging.\"\n",
        "]\n",
        "\n",
        "risk_map = {\n",
        "    \"Neg\": \"High Risk\",\n",
        "    \"Conf/Neu\": \"Medium Risk\",\n",
        "    \"Pos\": \"Low Risk\"\n",
        "}\n",
        "\n",
        "def make_combined_from_text_lr(texts):\n",
        "    \"\"\"TF-IDF + zero-padded extra features to match best_lr.n_features_in_\"\"\"\n",
        "    X_tfidf = tfidf_vectorizer.transform(texts)\n",
        "    n_model = best_lr.n_features_in_\n",
        "    n_tfidf = X_tfidf.shape[1]\n",
        "    n_extra = n_model - n_tfidf\n",
        "    if n_extra < 0:\n",
        "        raise ValueError(f\"TF-IDF has {n_tfidf} features, model expects {n_model}.\")\n",
        "    extra = csr_matrix((X_tfidf.shape[0], n_extra))\n",
        "    return hstack([X_tfidf, extra])\n",
        "\n",
        "# TF-IDF → combined → LogReg\n",
        "X_example_lr   = make_combined_from_text_lr(example_texts)\n",
        "example_probs_lr = best_lr.predict_proba(X_example_lr)\n",
        "example_pred_lr  = np.argmax(example_probs_lr, axis=1)\n",
        "\n",
        "print(\"=== REAL-TIME PREDICTIONS (Logistic Regression) ===\")\n",
        "for text, idx, prob in zip(example_texts, example_pred_lr, example_probs_lr):\n",
        "    label = label_names[idx]\n",
        "    risk  = risk_map[label]\n",
        "    print(\"\\nText:\", text)\n",
        "    print(\"Predicted label:\", label)\n",
        "    print(\"Risk Level:\", risk)\n",
        "    print(\"Class probabilities [Neg, Conf/Neu, Pos]:\", np.round(prob, 3))\n"
      ],
      "metadata": {
        "id": "h22nXJcn32ng"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### XAI 1 — Logistic Regression Coefficients & Odds Ratios"
      ],
      "metadata": {
        "id": "TmHw0awjw8Nn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# XAI 1 — Logistic Regression Coefficients & Odds Ratios\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# TF-IDF feature names\n",
        "feature_names_tfidf = tfidf_vectorizer.get_feature_names_out()\n",
        "\n",
        "# Align feature names with model's feature dimension\n",
        "n_model = best_lr.n_features_in_\n",
        "n_tfidf = len(feature_names_tfidf)\n",
        "\n",
        "if n_model > n_tfidf:\n",
        "    extra_names = np.array([f\"EXTRA_FEAT_{i+1}\" for i in range(n_model - n_tfidf)])\n",
        "    feature_names = np.concatenate([feature_names_tfidf, extra_names])\n",
        "else:\n",
        "    feature_names = feature_names_tfidf[:n_model]\n",
        "\n",
        "coefs = best_lr.coef_\n",
        "\n",
        "coef_df_list = []\n",
        "for i, class_name in enumerate(labels):\n",
        "    tmp = pd.DataFrame({\n",
        "        \"feature\": feature_names,\n",
        "        \"coef\": coefs[i],\n",
        "        \"odds_ratio\": np.exp(coefs[i]),\n",
        "        \"class\": class_name\n",
        "    })\n",
        "    coef_df_list.append(tmp)\n",
        "\n",
        "coef_df = pd.concat(coef_df_list, ignore_index=True)\n",
        "\n",
        "# Show top + bottom features per class\n",
        "for class_name in labels:\n",
        "    print(f\"Class: {class_name}\")\n",
        "    df_sub = coef_df[coef_df[\"class\"] == class_name]\n",
        "\n",
        "    print(\"\\nTop 15 POSITIVE features (push towards this class):\")\n",
        "    display(df_sub.sort_values(\"coef\", ascending=False).head(15))\n",
        "\n",
        "    print(\"\\nTop 15 NEGATIVE features (push away from this class):\")\n",
        "    display(df_sub.sort_values(\"coef\", ascending=True).head(15))\n"
      ],
      "metadata": {
        "id": "mPS0Tf76wivM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## XA2 Logistic Regression LIME"
      ],
      "metadata": {
        "id": "WIzMiL6cxXxv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# FIXED LIME (Local)\n",
        "from lime.lime_text import LimeTextExplainer\n",
        "from scipy.sparse import hstack, csr_matrix\n",
        "import numpy as np\n",
        "\n",
        "# shape-safe transformer\n",
        "def make_combined_from_text(texts):\n",
        "    X_tfidf = tfidf_vectorizer.transform(texts)\n",
        "\n",
        "    n_model = best_lr.n_features_in_\n",
        "    n_tfidf = X_tfidf.shape[1]\n",
        "    n_extra = n_model - n_tfidf\n",
        "\n",
        "    extra = csr_matrix(np.zeros((len(texts), n_extra)))\n",
        "    return hstack([X_tfidf, extra])\n",
        "\n",
        "def predict_proba_lime(texts):\n",
        "    return best_lr.predict_proba(make_combined_from_text(texts))\n",
        "\n",
        "explainer = LimeTextExplainer(class_names=labels)\n",
        "\n",
        "example_idx = 10\n",
        "text_example = X_val_text.iloc[example_idx]\n",
        "true_label = labels[y_val[example_idx]]\n",
        "\n",
        "print(\"TEXT:\", text_example)\n",
        "print(\"TRUE LABEL:\", true_label)\n",
        "\n",
        "exp = explainer.explain_instance(\n",
        "    text_example,\n",
        "    predict_proba_lime,\n",
        "    num_features=10\n",
        ")\n",
        "\n",
        "exp.show_in_notebook(text=True)\n"
      ],
      "metadata": {
        "id": "uhI0JlMBxDUS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Support Vector Machine (SVM)"
      ],
      "metadata": {
        "id": "zduOUye5rmwj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ================================\n",
        "# SVM with TF-IDF (TRAIN + EVAL)\n",
        "# ================================\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import (\n",
        "    classification_report, confusion_matrix, ConfusionMatrixDisplay,\n",
        "    roc_curve, auc, roc_auc_score\n",
        ")\n",
        "from sklearn.preprocessing import label_binarize\n",
        "from sklearn.calibration import CalibratedClassifierCV\n",
        "\n",
        "# Label names\n",
        "label_names = [id2label[i] for i in sorted(id2label.keys())]\n",
        "\n",
        "# ------------------------------------------------------------------\n",
        "# 1) Train Linear SVM on TF-IDF ONLY (no combined embeddings)\n",
        "# ------------------------------------------------------------------\n",
        "svm_base = LinearSVC(\n",
        "    class_weight=class_weight_dict,\n",
        "    random_state=RANDOM_SEED,\n",
        "    max_iter=3000\n",
        ")\n",
        "\n",
        "# small grid / almost no tuning so it's fast\n",
        "svm_param_grid = {\"C\": [1.0]}\n",
        "\n",
        "svm_grid = GridSearchCV(\n",
        "    estimator=svm_base,\n",
        "    param_grid=svm_param_grid,\n",
        "    scoring=\"f1_macro\",\n",
        "    cv=3,\n",
        "    n_jobs=-1,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "svm_grid.fit(X_train_tfidf, y_train)\n",
        "best_svm = svm_grid.best_estimator_\n",
        "\n",
        "print(\"Best SVM hyperparameters:\", svm_grid.best_params_)\n",
        "\n",
        "# ------------------------------------------------------------------\n",
        "# 2) Validation + Test reports\n",
        "# ------------------------------------------------------------------\n",
        "y_val_pred_svm  = best_svm.predict(X_val_tfidf)\n",
        "y_test_pred_svm = best_svm.predict(X_test_tfidf)\n",
        "\n",
        "print(\"\\n=== Linear SVM (val, tuned) ===\")\n",
        "print(classification_report(y_val, y_val_pred_svm, target_names=label_names))\n",
        "\n",
        "print(\"=== Linear SVM (test, tuned) ===\")\n",
        "print(classification_report(y_test, y_test_pred_svm, target_names=label_names))\n",
        "\n",
        "# ------------------------------------------------------------------\n",
        "# 3) Confusion matrix on test\n",
        "# ------------------------------------------------------------------\n",
        "cm_svm = confusion_matrix(y_test, y_test_pred_svm)\n",
        "print(\"\\nConfusion matrix (SVM, test):\")\n",
        "print(cm_svm)\n",
        "\n",
        "plt.figure(figsize=(5, 4))\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm_svm, display_labels=label_names)\n",
        "disp.plot(cmap=\"Purples\", values_format=\"d\")\n",
        "plt.title(\"Linear SVM – Confusion Matrix (Test)\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# ------------------------------------------------------------------\n",
        "# 4) Calibrated probabilities + ROC curves\n",
        "# ------------------------------------------------------------------\n",
        "svm_calibrated = CalibratedClassifierCV(best_svm, method=\"sigmoid\", cv=3)\n",
        "svm_calibrated.fit(X_train_tfidf, y_train)\n",
        "\n",
        "y_val_proba_svm  = svm_calibrated.predict_proba(X_val_tfidf)\n",
        "y_test_proba_svm = svm_calibrated.predict_proba(X_test_tfidf)\n",
        "\n",
        "print(\"Probabilities shapes (val, test):\", y_val_proba_svm.shape, y_test_proba_svm.shape)\n",
        "\n",
        "# Binarize labels for ROC\n",
        "y_val_bin  = label_binarize(y_val,  classes=[0, 1, 2])\n",
        "y_test_bin = label_binarize(y_test, classes=[0, 1, 2])\n",
        "\n",
        "# ROC on validation\n",
        "plt.figure(figsize=(6, 5))\n",
        "for i, name in enumerate(label_names):\n",
        "    fpr, tpr, _ = roc_curve(y_val_bin[:, i], y_val_proba_svm[:, i])\n",
        "    roc_auc = auc(fpr, tpr)\n",
        "    plt.plot(fpr, tpr, label=f\"{name} (AUC={roc_auc:.2f})\")\n",
        "\n",
        "plt.plot([0, 1], [0, 1], \"k--\", alpha=0.5)\n",
        "plt.title(\"SVM (Calibrated) – ROC Curve (Validation)\")\n",
        "plt.xlabel(\"False Positive Rate\")\n",
        "plt.ylabel(\"True Positive Rate\")\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Macro AUC\n",
        "svm_macro_auc_val  = roc_auc_score(y_val_bin,  y_val_proba_svm,  average=\"macro\")\n",
        "svm_macro_auc_test = roc_auc_score(y_test_bin, y_test_proba_svm, average=\"macro\")\n",
        "\n",
        "print(\"SVM (calibrated) macro AUC (val): \", svm_macro_auc_val)\n",
        "print(\"SVM (calibrated) macro AUC (test):\", svm_macro_auc_test)\n"
      ],
      "metadata": {
        "id": "nCMNafEayoaT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# REAL-TIME PREDICTIONS + RISK (Linear SVM Calibrated)\n",
        "# ==========================================\n",
        "example_texts = [\n",
        "    \"The lectures are poorly structured and I rarely understand the topics. Overall, this module feels frustrating.\",\n",
        "    \"Some weeks are clear, some are confusing. Overall it's okay but I am not fully confident.\",\n",
        "    \"The lecturer is amazing, explains everything clearly and the module is very engaging.\"\n",
        "]\n",
        "\n",
        "risk_map = {\n",
        "    \"Neg\": \"High Risk\",\n",
        "    \"Conf/Neu\": \"Medium Risk\",\n",
        "    \"Pos\": \"Low Risk\"\n",
        "}\n",
        "\n",
        "# TF-IDF → Calibrated SVM\n",
        "X_example_svm = tfidf_vectorizer.transform(example_texts)\n",
        "example_probs_svm = svm_calibrated.predict_proba(X_example_svm)\n",
        "example_pred_svm = np.argmax(example_probs_svm, axis=1)\n",
        "\n",
        "print(\"=== REAL-TIME PREDICTIONS (Linear SVM – Calibrated) ===\")\n",
        "for text, idx, prob in zip(example_texts, example_pred_svm, example_probs_svm):\n",
        "    label = label_names[idx]\n",
        "    risk  = risk_map[label]\n",
        "    print(\"\\nText:\", text)\n",
        "    print(\"Predicted label:\", label)\n",
        "    print(\"Risk Level:\", risk)\n",
        "    print(\"Class probabilities [Neg, Conf/Neu, Pos]:\", np.round(prob, 3))\n"
      ],
      "metadata": {
        "id": "W4HxDWei37md"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# XAI 1 — Global Feature Importance (SVM)\n",
        "# ==========================================\n",
        "import pandas as pd\n",
        "\n",
        "# Feature names from TF-IDF\n",
        "feature_names = tfidf_vectorizer.get_feature_names_out()\n",
        "\n",
        "coef = best_svm.coef_  # shape: (n_classes, n_features)\n",
        "\n",
        "def top_features_for_class(class_idx, k=15):\n",
        "    \"\"\"\n",
        "    Returns top positive and negative features for a given class index.\n",
        "    \"\"\"\n",
        "    class_name = label_names[class_idx]\n",
        "    w = coef[class_idx]\n",
        "\n",
        "    # Top positive\n",
        "    top_pos_idx = np.argsort(w)[-k:][::-1]\n",
        "    top_pos = pd.DataFrame({\n",
        "        \"feature\": feature_names[top_pos_idx],\n",
        "        \"coef\": w[top_pos_idx],\n",
        "        \"odds_ratio\": np.exp(w[top_pos_idx]),\n",
        "        \"class\": class_name\n",
        "    })\n",
        "\n",
        "    # Top negative\n",
        "    top_neg_idx = np.argsort(w)[:k]\n",
        "    top_neg = pd.DataFrame({\n",
        "        \"feature\": feature_names[top_neg_idx],\n",
        "        \"coef\": w[top_neg_idx],\n",
        "        \"odds_ratio\": np.exp(w[top_neg_idx]),\n",
        "        \"class\": class_name\n",
        "    })\n",
        "\n",
        "    return top_pos, top_neg\n",
        "\n",
        "for i, name in enumerate(label_names):\n",
        "    print(\"\\n===================================\")\n",
        "    print(f\"🔹 Class: {name}\")\n",
        "    print(\"===================================\\n\")\n",
        "\n",
        "    pos_df, neg_df = top_features_for_class(i, k=15)\n",
        "\n",
        "    print(\"Top 15 POSITIVE features (push towards this class):\\n\")\n",
        "    display(pos_df)\n",
        "\n",
        "    print(\"\\nTop 15 NEGATIVE features (push away from this class):\\n\")\n",
        "    display(neg_df)\n"
      ],
      "metadata": {
        "id": "RFJa8f91yoV9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================================\n",
        "# XAI 2 — LIME + REAL-TIME DEMO + RISK LEVEL (SVM)\n",
        "# ==========================================================\n",
        "from lime.lime_text import LimeTextExplainer\n",
        "\n",
        "# 1) Prediction function for LIME (uses TF-IDF + calibrated SVM)\n",
        "def predict_proba_lime(texts):\n",
        "    X = tfidf_vectorizer.transform(texts)\n",
        "    return svm_calibrated.predict_proba(X)\n",
        "\n",
        "explainer_svm = LimeTextExplainer(class_names=label_names)\n",
        "\n",
        "# Choose an example from validation set\n",
        "example_idx = 10  # change index if needed\n",
        "text_example = X_val_text.iloc[example_idx]\n",
        "\n",
        "# y_val is a NumPy array\n",
        "true_label = label_names[int(y_val[example_idx])]\n",
        "\n",
        "print(f\"Explaining text (index {example_idx})\")\n",
        "print(\"True label:\", true_label)\n",
        "print(\"\\nText:\\n\", text_example)\n",
        "\n",
        "exp_svm = explainer_svm.explain_instance(\n",
        "    text_example,\n",
        "    predict_proba_lime,\n",
        "    num_features=10,\n",
        "    labels=list(range(len(label_names)))   # <-- ask for ALL classes\n",
        ")\n",
        "\n",
        "# Print contributions per class\n",
        "for class_idx, class_name in enumerate(label_names):\n",
        "    print(f\"\\n--- LIME explanation for {class_name} ---\")\n",
        "    for word, weight in exp_svm.as_list(label=class_idx):\n",
        "        print(f\"{word:25s} {weight:+.4f}\")\n",
        "\n",
        "# Render HTML in notebook\n",
        "exp_svm.show_in_notebook(text=True)\n",
        "\n",
        "\n",
        "# ----------------------------------------------------------\n",
        "# 2) REAL-TIME PREDICTIONS + RISK LEVEL (using the same SVM)\n",
        "# ----------------------------------------------------------\n",
        "example_texts = [\n",
        "    \"The lectures are poorly structured and I rarely understand the topics. Overall, this module feels frustrating.\",\n",
        "    \"Some weeks are clear, some are confusing, overall it's okay but I am not fully confident.\",\n",
        "    \"The lecturer is amazing, explains everything clearly and the module is very engaging.\"\n",
        "]\n",
        "\n",
        "# Risk mapping\n",
        "risk_map = {\n",
        "    \"Neg\": \"High Risk\",\n",
        "    \"Conf/Neu\": \"Medium Risk\",\n",
        "    \"Pos\": \"Low Risk\"\n",
        "}\n",
        "\n",
        "# Predict labels + probabilities\n",
        "X_example = tfidf_vectorizer.transform(example_texts)\n",
        "example_probs = svm_calibrated.predict_proba(X_example)\n",
        "example_pred_idx = np.argmax(example_probs, axis=1)\n",
        "\n",
        "print(\"\\n=== REAL-TIME PREDICTIONS WITH RISK LEVEL (Linear SVM) ===\")\n",
        "for text, idx, prob in zip(example_texts, example_pred_idx, example_probs):\n",
        "    label = label_names[idx]\n",
        "    risk  = risk_map[label]\n",
        "    print(\"\\nText:\", text)\n",
        "    print(\"Predicted label:\", label)\n",
        "    print(\"Risk Level:\", risk)\n",
        "    print(\"Class probabilities [Neg, Conf/Neu, Pos]:\", np.round(prob, 3))\n"
      ],
      "metadata": {
        "id": "dQ_OhR1jyoOP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mIp9s2VF45d1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**DEEP LEARNING SETUP**"
      ],
      "metadata": {
        "id": "gj8ccWjJ8rvK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ANN – SVD features + training\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "\n",
        "RANDOM_SEED = 42\n",
        "np.random.seed(RANDOM_SEED)\n",
        "tf.random.set_seed(RANDOM_SEED)\n",
        "\n",
        "# 1) Reduce TF-IDF dimensionality for ANN\n",
        "n_components = 300   # good trade-off (can change to 200/400)\n",
        "svd_ann = TruncatedSVD(\n",
        "    n_components=n_components,\n",
        "    random_state=RANDOM_SEED\n",
        ")\n",
        "\n",
        "X_train_ann = svd_ann.fit_transform(X_train_tfidf)\n",
        "X_val_ann   = svd_ann.transform(X_val_tfidf)\n",
        "X_test_ann  = svd_ann.transform(X_test_tfidf)\n",
        "\n",
        "print(\"SVD shapes:\")\n",
        "print(\"  Train:\", X_train_ann.shape)\n",
        "print(\"  Val  :\", X_val_ann.shape)\n",
        "print(\"  Test :\", X_test_ann.shape)\n",
        "\n",
        "num_features = n_components\n",
        "num_classes  = 3\n",
        "\n",
        "# 2) Build a compact ANN (fast & light)\n",
        "ann_model = models.Sequential([\n",
        "    layers.Input(shape=(num_features,)),\n",
        "    layers.Dense(256, activation='relu'),\n",
        "    layers.Dropout(0.3),\n",
        "    layers.Dense(128, activation='relu'),\n",
        "    layers.Dropout(0.2),\n",
        "    layers.Dense(num_classes, activation='softmax')\n",
        "])\n",
        "\n",
        "ann_model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),\n",
        "    loss=\"sparse_categorical_crossentropy\",\n",
        "    metrics=[\"accuracy\"]\n",
        ")\n",
        "\n",
        "ann_model.summary()\n",
        "\n",
        "# 3) Train\n",
        "callback = tf.keras.callbacks.EarlyStopping(\n",
        "    monitor='val_loss',\n",
        "    patience=1,\n",
        "    restore_best_weights=True\n",
        ")\n",
        "\n",
        "history_ann = ann_model.fit(\n",
        "    X_train_ann, y_train,\n",
        "    validation_data=(X_val_ann, y_val),\n",
        "    epochs=6,\n",
        "    batch_size=256,\n",
        "    callbacks=[callback],\n",
        "    verbose=1\n",
        ")\n"
      ],
      "metadata": {
        "id": "AzPpztLT5qon"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ANN – EVAL + LIME + RISK DEMO\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import (\n",
        "    classification_report,\n",
        "    confusion_matrix,\n",
        "    ConfusionMatrixDisplay,\n",
        "    roc_curve, auc, roc_auc_score\n",
        ")\n",
        "from sklearn.preprocessing import label_binarize\n",
        "from lime.lime_text import LimeTextExplainer\n",
        "\n",
        "label_names = [\"Neg\", \"Conf/Neu\", \"Pos\"]\n",
        "\n",
        "# 1) Predictions (val + test)\n",
        "y_val_proba_ann  = ann_model.predict(X_val_ann)\n",
        "y_test_proba_ann = ann_model.predict(X_test_ann)\n",
        "\n",
        "y_val_pred_ann  = np.argmax(y_val_proba_ann, axis=1)\n",
        "y_test_pred_ann = np.argmax(y_test_proba_ann, axis=1)\n",
        "\n",
        "print(\"=== ANN (val) ===\")\n",
        "print(classification_report(y_val, y_val_pred_ann, target_names=label_names))\n",
        "\n",
        "print(\"=== ANN (test) ===\")\n",
        "print(classification_report(y_test, y_test_pred_ann, target_names=label_names))\n",
        "\n",
        "# 2) Confusion matrix (test)\n",
        "cm_ann = confusion_matrix(y_test, y_test_pred_ann)\n",
        "print(\"\\nConfusion matrix (ANN, test):\")\n",
        "print(cm_ann)\n",
        "\n",
        "plt.figure(figsize=(5, 4))\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm_ann, display_labels=label_names)\n",
        "disp.plot(cmap=\"Greens\", values_format=\"d\")\n",
        "plt.title(\"ANN – Confusion Matrix (Test)\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# 3) ROC (macro AUC)\n",
        "y_val_bin  = label_binarize(y_val,  classes=[0, 1, 2])\n",
        "y_test_bin = label_binarize(y_test, classes=[0, 1, 2])\n",
        "\n",
        "plt.figure(figsize=(6, 5))\n",
        "for i, name in enumerate(label_names):\n",
        "    fpr, tpr, _ = roc_curve(y_val_bin[:, i], y_val_proba_ann[:, i])\n",
        "    roc_auc = auc(fpr, tpr)\n",
        "    plt.plot(fpr, tpr, label=f\"{name} (AUC={roc_auc:.2f})\")\n",
        "\n",
        "plt.plot([0, 1], [0, 1], \"k--\", alpha=0.5)\n",
        "plt.title(\"ANN – ROC Curve (Validation)\")\n",
        "plt.xlabel(\"False Positive Rate\")\n",
        "plt.ylabel(\"True Positive Rate\")\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "ann_macro_auc_val  = roc_auc_score(y_val_bin,  y_val_proba_ann,  average=\"macro\")\n",
        "ann_macro_auc_test = roc_auc_score(y_test_bin, y_test_proba_ann, average=\"macro\")\n",
        "print(\"ANN macro AUC (val): \", ann_macro_auc_val)\n",
        "print(\"ANN macro AUC (test):\", ann_macro_auc_test)\n",
        "\n",
        "# 4) LIME for ANN – local explanation on one review\n",
        "def predict_proba_ann_lime(texts):\n",
        "    X_tfidf = tfidf_vectorizer.transform(texts)\n",
        "    X_svd   = svd_ann.transform(X_tfidf)\n",
        "    return ann_model.predict(X_svd)\n",
        "\n",
        "explainer_ann = LimeTextExplainer(class_names=label_names)\n",
        "\n",
        "example_idx = 10\n",
        "text_example = X_val_text.iloc[example_idx]\n",
        "true_label   = label_names[int(y_val[example_idx])]\n",
        "\n",
        "print(f\"\\nExplaining text (index {example_idx})\")\n",
        "print(\"True label:\", true_label)\n",
        "print(\"\\nText:\\n\", text_example)\n",
        "\n",
        "exp_ann = explainer_ann.explain_instance(\n",
        "    text_example,\n",
        "    predict_proba_ann_lime,\n",
        "    num_features=10\n",
        ")\n",
        "\n",
        "exp_ann.show_in_notebook(text=True)\n",
        "\n",
        "# 5) REAL-TIME DEMO + RISK\n",
        "example_texts = [\n",
        "    \"The lectures are poorly structured and I rarely understand the topics. Overall, this module feels frustrating.\",\n",
        "    \"Some weeks are clear, some are confusing, overall it's okay but I am not fully confident.\",\n",
        "    \"The lecturer is amazing, explains everything clearly and the module is very engaging.\"\n",
        "]\n",
        "\n",
        "risk_map = {\n",
        "    \"Neg\": \"High Risk\",\n",
        "    \"Conf/Neu\": \"Medium Risk\",\n",
        "    \"Pos\": \"Low Risk\"\n",
        "}\n",
        "\n",
        "X_example_tfidf = tfidf_vectorizer.transform(example_texts)\n",
        "X_example_ann   = svd_ann.transform(X_example_tfidf)\n",
        "example_probs   = ann_model.predict(X_example_ann)\n",
        "example_pred    = np.argmax(example_probs, axis=1)\n",
        "\n",
        "print(\"\\n=== REAL-TIME PREDICTIONS WITH RISK LEVEL (ANN) ===\")\n",
        "for text, idx, prob in zip(example_texts, example_pred, example_probs):\n",
        "    label = label_names[idx]\n",
        "    risk  = risk_map[label]\n",
        "    print(\"\\nText:\", text)\n",
        "    print(\"Predicted label:\", label)\n",
        "    print(\"Risk Level:\", risk)\n",
        "    print(\"Class probabilities [Neg, Conf/Neu, Pos]:\", np.round(prob, 3))\n"
      ],
      "metadata": {
        "id": "4jJwNlVi492C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ANN – REAL-TIME PREDICTIONS + RISK LEVEL\n",
        "import numpy as np\n",
        "\n",
        "label_names = [\"Neg\", \"Conf/Neu\", \"Pos\"]\n",
        "\n",
        "risk_map = {\n",
        "    \"Neg\": \"High Risk\",\n",
        "    \"Conf/Neu\": \"Medium Risk\",\n",
        "    \"Pos\": \"Low Risk\",\n",
        "}\n",
        "\n",
        "# Example raw reviews (change these for viva)\n",
        "example_texts = [\n",
        "    \"The lectures are poorly structured and I rarely understand the topics. Overall, this module feels frustrating.\",\n",
        "    \"Some weeks are clear, some are confusing, overall it's okay but I am not fully confident.\",\n",
        "    \"The lecturer is amazing, explains everything clearly and the module is very engaging.\"\n",
        "]\n",
        "\n",
        "def basic_clean(t):\n",
        "    return t.lower().strip()\n",
        "\n",
        "# 1) Text → TF-IDF → SVD features (same pipeline as training)\n",
        "example_clean = [basic_clean(t) for t in example_texts]\n",
        "X_ex_tfidf = tfidf_vectorizer.transform(example_clean)\n",
        "X_ex_ann   = svd_ann.transform(X_ex_tfidf)\n",
        "\n",
        "# 2) ANN probabilities + predicted labels\n",
        "probs_ann = ann_model.predict(X_ex_ann)\n",
        "pred_idx  = np.argmax(probs_ann, axis=1)\n",
        "\n",
        "print(\"\\n=== REAL-TIME PREDICTIONS WITH RISK LEVEL (ANN) ===\")\n",
        "for text, idx, p in zip(example_texts, pred_idx, probs_ann):\n",
        "    label = label_names[idx]\n",
        "    risk  = risk_map[label]\n",
        "    print(\"\\nText:\", text)\n",
        "    print(\"Predicted label:\", label)\n",
        "    print(\"Risk Level:\", risk)\n",
        "    print(\"Class probabilities [Neg, Conf/Neu, Pos]:\", np.round(p, 3))\n"
      ],
      "metadata": {
        "id": "BcC-Gm8L49yx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TRAINING (BiLSTM)\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras import layers, models\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "\n",
        "# Reuse random seed + splits + label dicts you already created earlier\n",
        "tf.random.set_seed(RANDOM_SEED)\n",
        "np.random.seed(RANDOM_SEED)\n",
        "\n",
        "# Text splits (same as classical models)\n",
        "X_train_text = df_fe.loc[train_idx, \"text_clean\"].astype(str).tolist()\n",
        "X_val_text   = df_fe.loc[val_idx,   \"text_clean\"].astype(str).tolist()\n",
        "X_test_text  = df_fe.loc[test_idx,  \"text_clean\"].astype(str).tolist()\n",
        "\n",
        "y_train_dl = y_train.copy()\n",
        "y_val_dl   = y_val.copy()\n",
        "y_test_dl  = y_test.copy()\n",
        "\n",
        "# Tokenisation / padding\n",
        "MAX_WORDS = 30000\n",
        "MAX_LEN   = 150\n",
        "\n",
        "tokenizer = Tokenizer(num_words=MAX_WORDS, oov_token=\"<UNK>\")\n",
        "tokenizer.fit_on_texts(X_train_text)\n",
        "\n",
        "X_train_seq = tokenizer.texts_to_sequences(X_train_text)\n",
        "X_val_seq   = tokenizer.texts_to_sequences(X_val_text)\n",
        "X_test_seq  = tokenizer.texts_to_sequences(X_test_text)\n",
        "\n",
        "X_train_pad = pad_sequences(X_train_seq, maxlen=MAX_LEN, padding=\"post\", truncating=\"post\")\n",
        "X_val_pad   = pad_sequences(X_val_seq,   maxlen=MAX_LEN, padding=\"post\", truncating=\"post\")\n",
        "X_test_pad  = pad_sequences(X_test_seq,  maxlen=MAX_LEN, padding=\"post\", truncating=\"post\")\n",
        "\n",
        "vocab_size = min(MAX_WORDS, len(tokenizer.word_index) + 1)\n",
        "print(\"Vocab size:\", vocab_size)\n",
        "print(\"Train seq shape:\", X_train_pad.shape)\n",
        "print(\"Val seq shape  :\", X_val_pad.shape)\n",
        "print(\"Test seq shape :\", X_test_pad.shape)\n",
        "print(\"id2label:\", id2label)\n",
        "print(\"class_weight_dict:\", class_weight_dict)\n",
        "\n",
        "# ---------- BiLSTM model ----------\n",
        "\n",
        "def build_bilstm_model(vocab_size, max_len, n_classes=3, emb_dim=128, lstm_units=64):\n",
        "    model = models.Sequential()\n",
        "    model.add(layers.Embedding(\n",
        "        input_dim=vocab_size,\n",
        "        output_dim=emb_dim,\n",
        "        input_shape=(max_len,)      # avoids \"unbuilt\" summary + deprecation warning\n",
        "    ))\n",
        "    model.add(layers.Bidirectional(layers.LSTM(lstm_units, return_sequences=True)))\n",
        "    model.add(layers.GlobalMaxPooling1D())\n",
        "    model.add(layers.Dense(64, activation=\"relu\"))\n",
        "    model.add(layers.Dropout(0.3))\n",
        "    model.add(layers.Dense(n_classes, activation=\"softmax\"))\n",
        "\n",
        "    model.compile(\n",
        "        loss=\"sparse_categorical_crossentropy\",\n",
        "        optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),\n",
        "        metrics=[\"accuracy\"],\n",
        "    )\n",
        "    return model\n",
        "\n",
        "bilstm_model = build_bilstm_model(\n",
        "    vocab_size=vocab_size,\n",
        "    max_len=MAX_LEN,\n",
        "    n_classes=len(label2id)\n",
        ")\n",
        "\n",
        "# build explicitly so summary shows params\n",
        "bilstm_model.build(input_shape=(None, MAX_LEN))\n",
        "bilstm_model.summary()\n",
        "\n",
        "bilstm_ckpt = \"bilstm_best.keras\"\n",
        "\n",
        "es = EarlyStopping(\n",
        "    monitor=\"val_loss\",\n",
        "    patience=3,\n",
        "    restore_best_weights=True,\n",
        "    verbose=1,\n",
        ")\n",
        "\n",
        "mc = ModelCheckpoint(\n",
        "    filepath=bilstm_ckpt,\n",
        "    monitor=\"val_loss\",\n",
        "    save_best_only=True,\n",
        "    verbose=1,\n",
        ")\n",
        "\n",
        "history_bilstm = bilstm_model.fit(\n",
        "    X_train_pad,\n",
        "    y_train_dl,\n",
        "    validation_data=(X_val_pad, y_val_dl),\n",
        "    epochs=10,\n",
        "    batch_size=128,\n",
        "    class_weight=class_weight_dict,\n",
        "    callbacks=[es, mc],\n",
        "    verbose=1,\n",
        ")\n"
      ],
      "metadata": {
        "id": "pmsjovgiFZdm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix, roc_curve, auc\n",
        "from sklearn.preprocessing import label_binarize\n",
        "\n",
        "# reload best BiLSTM weights\n",
        "bilstm_model.load_weights(bilstm_ckpt)\n",
        "\n",
        "label_names = [id2label[i] for i in range(len(label2id))]\n",
        "\n",
        "# ===== Validation =====\n",
        "y_val_probs_bilstm = bilstm_model.predict(X_val_pad, batch_size=256, verbose=1)\n",
        "y_val_pred_bilstm  = np.argmax(y_val_probs_bilstm, axis=1)\n",
        "\n",
        "print(\"=== BiLSTM – Validation ===\")\n",
        "print(classification_report(y_val_dl, y_val_pred_bilstm, target_names=label_names))\n",
        "\n",
        "cm_val_bilstm = confusion_matrix(y_val_dl, y_val_pred_bilstm)\n",
        "plt.figure(figsize=(5,4))\n",
        "sns.heatmap(cm_val_bilstm, annot=True, fmt=\"d\",\n",
        "            xticklabels=label_names, yticklabels=label_names)\n",
        "plt.title(\"BiLSTM – Validation Confusion Matrix\")\n",
        "plt.xlabel(\"Predicted\"); plt.ylabel(\"True\")\n",
        "plt.tight_layout(); plt.show()\n",
        "\n",
        "# ===== Test =====\n",
        "y_test_probs_bilstm = bilstm_model.predict(X_test_pad, batch_size=256, verbose=1)\n",
        "y_test_pred_bilstm  = np.argmax(y_test_probs_bilstm, axis=1)\n",
        "\n",
        "print(\"\\n=== BiLSTM – Test ===\")\n",
        "print(classification_report(y_test_dl, y_test_pred_bilstm, target_names=label_names))\n",
        "\n",
        "cm_test_bilstm = confusion_matrix(y_test_dl, y_test_pred_bilstm)\n",
        "plt.figure(figsize=(5,4))\n",
        "sns.heatmap(cm_test_bilstm, annot=True, fmt=\"d\",\n",
        "            xticklabels=label_names, yticklabels=label_names)\n",
        "plt.title(\"BiLSTM – Test Confusion Matrix\")\n",
        "plt.xlabel(\"Predicted\"); plt.ylabel(\"True\")\n",
        "plt.tight_layout(); plt.show()\n",
        "\n",
        "# ===== ROC Curve (multiclass, test set) =====\n",
        "y_test_true = y_test_dl\n",
        "y_test_bin  = label_binarize(y_test_true, classes=[0,1,2])\n",
        "fpr, tpr, roc_auc = {}, {}, {}\n",
        "\n",
        "for i in [0,1,2]:\n",
        "    fpr[i], tpr[i], _ = roc_curve(y_test_bin[:, i], y_test_probs_bilstm[:, i])\n",
        "    roc_auc[i] = auc(fpr[i], tpr[i])\n",
        "\n",
        "all_fpr = np.unique(np.concatenate([fpr[i] for i in [0,1,2]]))\n",
        "mean_tpr = np.zeros_like(all_fpr)\n",
        "for i in [0,1,2]:\n",
        "    mean_tpr += np.interp(all_fpr, fpr[i], tpr[i])\n",
        "mean_tpr /= 3\n",
        "roc_auc_macro = auc(all_fpr, mean_tpr)\n",
        "\n",
        "plt.figure(figsize=(8,6))\n",
        "colors = [\"red\", \"orange\", \"green\"]\n",
        "for i, c in zip([0,1,2], colors):\n",
        "    plt.plot(fpr[i], tpr[i], c, lw=2,\n",
        "             label=f\"{id2label[i]} AUC={roc_auc[i]:.3f}\")\n",
        "\n",
        "plt.plot(all_fpr, mean_tpr, \"b--\", lw=2,\n",
        "         label=f\"Macro AUC={roc_auc_macro:.3f}\")\n",
        "plt.plot([0,1],[0,1],\"k--\")\n",
        "plt.xlim([0,1]); plt.ylim([0,1.05])\n",
        "plt.xlabel(\"False Positive Rate\")\n",
        "plt.ylabel(\"True Positive Rate\")\n",
        "plt.title(\"ROC Curve — BiLSTM (Multiclass)\")\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.grid(alpha=0.3)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "AW_SUC27ZjOX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# BiLSTM — LIME TEXT XAI + REAL-TIME PREDICTIONS + RISK\n",
        "from lime.lime_text import LimeTextExplainer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import numpy as np\n",
        "\n",
        "label_names = [\"Neg\", \"Conf/Neu\", \"Pos\"]\n",
        "risk_map = {\n",
        "    \"Neg\": \"High Risk\",\n",
        "    \"Conf/Neu\": \"Medium Risk\",\n",
        "    \"Pos\": \"Low Risk\",\n",
        "}\n",
        "\n",
        "MAX_LEN = 150  # same as used in your BiLSTM training\n",
        "\n",
        "# ---------- 1) LIME wrapper for BiLSTM ----------\n",
        "def bilstm_predict_proba(texts):\n",
        "    seqs = tokenizer.texts_to_sequences(texts)\n",
        "    pads = pad_sequences(seqs, maxlen=MAX_LEN, padding=\"post\", truncating=\"post\")\n",
        "    return bilstm_model.predict(pads)\n",
        "\n",
        "explainer_bilstm = LimeTextExplainer(class_names=label_names)\n",
        "\n",
        "# Pick one validation example to explain\n",
        "example_idx = 10  # change if you want\n",
        "y_val_array = np.array(y_val_dl)\n",
        "text_example = X_val_text[example_idx]\n",
        "true_label  = label_names[int(y_val_array[example_idx])]\n",
        "\n",
        "print(f\"=== BiLSTM — LIME EXPLANATION (val index {example_idx}) ===\")\n",
        "print(\"True label:\", true_label)\n",
        "print(\"\\nText:\\n\", text_example)\n",
        "\n",
        "exp_bilstm = explainer_bilstm.explain_instance(\n",
        "    text_example,\n",
        "    bilstm_predict_proba,\n",
        "    num_features=10\n",
        ")\n",
        "\n",
        "# Nice HTML view (with highlighted tokens)\n",
        "exp_bilstm.show_in_notebook(text=True)\n",
        "\n",
        "# ---------- 2) REAL-TIME PREDICTIONS + RISK (BiLSTM) ----------\n",
        "example_texts = [\n",
        "    \"The lectures are poorly structured and I rarely understand the topics. Overall, this module feels frustrating.\",\n",
        "    \"Some weeks are clear, some are confusing, overall it's okay but I am not fully confident.\",\n",
        "    \"The lecturer is amazing, explains everything clearly and the module is very engaging.\"\n",
        "]\n",
        "\n",
        "seqs_ex = tokenizer.texts_to_sequences(example_texts)\n",
        "pads_ex = pad_sequences(seqs_ex, maxlen=MAX_LEN, padding=\"post\", truncating=\"post\")\n",
        "probs_ex = bilstm_model.predict(pads_ex)\n",
        "pred_idx = np.argmax(probs_ex, axis=1)\n",
        "\n",
        "print(\"\\n=== REAL-TIME PREDICTIONS + RISK (BiLSTM) ===\")\n",
        "for text, idx, prob in zip(example_texts, pred_idx, probs_ex):\n",
        "    label = label_names[int(idx)]\n",
        "    risk  = risk_map[label]\n",
        "    print(\"\\nText:\", text)\n",
        "    print(\"Predicted label:\", label)\n",
        "    print(\"Risk level:\", risk)\n",
        "    print(\"Class probabilities [Neg, Conf/Neu, Pos]:\", np.round(prob, 3))\n"
      ],
      "metadata": {
        "id": "bvrQs1cZAKSF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#  CNN MODEL: BUILD + TRAIN\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "\n",
        "tf.random.set_seed(RANDOM_SEED)\n",
        "np.random.seed(RANDOM_SEED)\n",
        "\n",
        "n_classes = len(id2label)\n",
        "\n",
        "def build_cnn_model(vocab_size, max_len, n_classes=3,\n",
        "                    emb_dim=128, num_filters=128, kernel_size=3):\n",
        "    model = models.Sequential()\n",
        "    model.add(layers.Embedding(\n",
        "        input_dim=vocab_size,\n",
        "        output_dim=emb_dim,\n",
        "        input_shape=(max_len,)\n",
        "    ))\n",
        "    model.add(layers.Conv1D(\n",
        "        filters=num_filters,\n",
        "        kernel_size=kernel_size,\n",
        "        activation=\"relu\"\n",
        "    ))\n",
        "    model.add(layers.GlobalMaxPooling1D())\n",
        "    model.add(layers.Dense(64, activation=\"relu\"))\n",
        "    model.add(layers.Dropout(0.3))\n",
        "    model.add(layers.Dense(n_classes, activation=\"softmax\"))\n",
        "\n",
        "    model.compile(\n",
        "        loss=\"sparse_categorical_crossentropy\",\n",
        "        optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),\n",
        "        metrics=[\"accuracy\"],\n",
        "    )\n",
        "    return model\n",
        "\n",
        "cnn_model = build_cnn_model(\n",
        "    vocab_size=vocab_size,\n",
        "    max_len=MAX_LEN,\n",
        "    n_classes=n_classes\n",
        ")\n",
        "\n",
        "cnn_model.build(input_shape=(None, MAX_LEN))\n",
        "cnn_model.summary()\n",
        "\n",
        "cnn_ckpt = \"cnn_best.keras\"\n",
        "\n",
        "es_cnn = EarlyStopping(\n",
        "    monitor=\"val_loss\",\n",
        "    patience=3,\n",
        "    restore_best_weights=True,\n",
        "    verbose=1,\n",
        ")\n",
        "\n",
        "mc_cnn = ModelCheckpoint(\n",
        "    filepath=cnn_ckpt,\n",
        "    monitor=\"val_loss\",\n",
        "    save_best_only=True,\n",
        "    verbose=1,\n",
        ")\n",
        "\n",
        "history_cnn = cnn_model.fit(\n",
        "    X_train_pad,\n",
        "    y_train_dl,\n",
        "    validation_data=(X_val_pad, y_val_dl),\n",
        "    epochs=10,\n",
        "    batch_size=128,\n",
        "    class_weight=class_weight_dict,\n",
        "    callbacks=[es_cnn, mc_cnn],\n",
        "    verbose=1,\n",
        ")\n"
      ],
      "metadata": {
        "id": "0x1D8BnzJHag"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix, roc_curve, auc\n",
        "from sklearn.preprocessing import label_binarize\n",
        "\n",
        "# Validation evaluation\n",
        "y_val_probs_cnn = cnn_model.predict(X_val_pad, batch_size=256, verbose=1)\n",
        "y_val_pred_cnn = np.argmax(y_val_probs_cnn, axis=1)\n",
        "\n",
        "print(\"=== CNN – Validation ===\")\n",
        "print(classification_report(y_val_dl, y_val_pred_cnn, target_names=[id2label[0], id2label[1], id2label[2]]))\n",
        "\n",
        "cm_val = confusion_matrix(y_val_dl, y_val_pred_cnn)\n",
        "plt.figure(figsize=(5,4))\n",
        "sns.heatmap(cm_val, annot=True, fmt=\"d\",\n",
        "            xticklabels=[id2label[0],id2label[1],id2label[2]],\n",
        "            yticklabels=[id2label[0],id2label[1],id2label[2]])\n",
        "plt.title(\"CNN – Validation Confusion Matrix\")\n",
        "plt.xlabel(\"Predicted\"); plt.ylabel(\"True\")\n",
        "plt.tight_layout(); plt.show()\n",
        "\n",
        "# Test evaluation\n",
        "y_test_probs_cnn = cnn_model.predict(X_test_pad, batch_size=256, verbose=1)\n",
        "y_test_pred_cnn = np.argmax(y_test_probs_cnn, axis=1)\n",
        "\n",
        "print(\"\\n=== CNN – Test ===\")\n",
        "print(classification_report(y_test_dl, y_test_pred_cnn, target_names=[id2label[0],id2label[1],id2label[2]]))\n",
        "\n",
        "cm_test = confusion_matrix(y_test_dl, y_test_pred_cnn)\n",
        "plt.figure(figsize=(5,4))\n",
        "sns.heatmap(cm_test, annot=True, fmt=\"d\",\n",
        "            xticklabels=[id2label[0],id2label[1],id2label[2]],\n",
        "            yticklabels=[id2label[0],id2label[1],id2label[2]])\n",
        "plt.title(\"CNN – Test Confusion Matrix\")\n",
        "plt.xlabel(\"Predicted\"); plt.ylabel(\"True\")\n",
        "plt.tight_layout(); plt.show()\n",
        "\n",
        "# ROC Curve\n",
        "y_bin = label_binarize(y_test_dl, classes=[0,1,2])\n",
        "fpr, tpr, roc_auc = {}, {}, {}\n",
        "\n",
        "for i in [0,1,2]:\n",
        "    fpr[i], tpr[i], _ = roc_curve(y_bin[:,i], y_test_probs_cnn[:,i])\n",
        "    roc_auc[i] = auc(fpr[i], tpr[i])\n",
        "\n",
        "all_fpr = np.unique(np.concatenate([fpr[i] for i in [0,1,2]]))\n",
        "mean_tpr = np.zeros_like(all_fpr)\n",
        "for i in [0,1,2]:\n",
        "    mean_tpr += np.interp(all_fpr, fpr[i], tpr[i])\n",
        "mean_tpr /= 3\n",
        "roc_auc_macro = auc(all_fpr, mean_tpr)\n",
        "\n",
        "plt.figure(figsize=(8,6))\n",
        "colors = [\"red\",\"orange\",\"green\"]\n",
        "labels = [id2label[0], id2label[1], id2label[2]]\n",
        "\n",
        "for i, c in zip([0,1,2], colors):\n",
        "    plt.plot(fpr[i], tpr[i], c, lw=2, label=f\"{labels[i]} AUC={roc_auc[i]:.3f}\")\n",
        "\n",
        "plt.plot(all_fpr, mean_tpr, \"b--\", lw=2, label=f\"Macro AUC={roc_auc_macro:.3f}\")\n",
        "plt.plot([0,1],[0,1],\"k--\"); plt.xlim([0,1]); plt.ylim([0,1.05])\n",
        "plt.xlabel(\"False Positive Rate\"); plt.ylabel(\"True Positive Rate\")\n",
        "plt.title(\"ROC Curve — CNN (Multiclass)\")\n",
        "plt.legend(loc=\"lower right\"); plt.grid(alpha=0.3)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "JkccvV8LJM1b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CNN — LIME TEXT XAI + REAL-TIME PREDICTIONS + RISK\n",
        "from lime.lime_text import LimeTextExplainer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import numpy as np\n",
        "\n",
        "label_names = [\"Neg\", \"Conf/Neu\", \"Pos\"]\n",
        "risk_map = {\n",
        "    \"Neg\": \"High Risk\",\n",
        "    \"Conf/Neu\": \"Medium Risk\",\n",
        "    \"Pos\": \"Low Risk\",\n",
        "}\n",
        "\n",
        "MAX_LEN = 150  # same as used in CNN training\n",
        "\n",
        "# ---------- 1) LIME wrapper for CNN ----------\n",
        "def cnn_predict_proba(texts):\n",
        "    seqs = tokenizer.texts_to_sequences(texts)\n",
        "    pads = pad_sequences(seqs, maxlen=MAX_LEN, padding=\"post\", truncating=\"post\")\n",
        "    return cnn_model.predict(pads)\n",
        "\n",
        "explainer_cnn = LimeTextExplainer(class_names=label_names)\n",
        "\n",
        "# Pick one validation example to explain\n",
        "example_idx = 10  # change if you want\n",
        "y_val_array = np.array(y_val_dl)\n",
        "text_example = X_val_text[example_idx]\n",
        "true_label  = label_names[int(y_val_array[example_idx])]\n",
        "\n",
        "print(f\"=== CNN — LIME EXPLANATION (val index {example_idx}) ===\")\n",
        "print(\"True label:\", true_label)\n",
        "print(\"\\nText:\\n\", text_example)\n",
        "\n",
        "exp_cnn = explainer_cnn.explain_instance(\n",
        "    text_example,\n",
        "    cnn_predict_proba,\n",
        "    num_features=10\n",
        ")\n",
        "\n",
        "exp_cnn.show_in_notebook(text=True)\n",
        "\n",
        "# ---------- 2) REAL-TIME PREDICTIONS + RISK (CNN) ----------\n",
        "example_texts = [\n",
        "    \"The lectures are poorly structured and I rarely understand the topics. Overall, this module feels frustrating.\",\n",
        "    \"Some weeks are clear, some are confusing, overall it's okay but I am not fully confident.\",\n",
        "    \"The lecturer is amazing, explains everything clearly and the module is very engaging.\"\n",
        "]\n",
        "\n",
        "seqs_ex = tokenizer.texts_to_sequences(example_texts)\n",
        "pads_ex = pad_sequences(seqs_ex, maxlen=MAX_LEN, padding=\"post\", truncating=\"post\")\n",
        "probs_ex = cnn_model.predict(pads_ex)\n",
        "pred_idx = np.argmax(probs_ex, axis=1)\n",
        "\n",
        "print(\"\\n=== REAL-TIME PREDICTIONS + RISK (CNN) ===\")\n",
        "for text, idx, prob in zip(example_texts, pred_idx, probs_ex):\n",
        "    label = label_names[int(idx)]\n",
        "    risk  = risk_map[label]\n",
        "    print(\"\\nText:\", text)\n",
        "    print(\"Predicted label:\", label)\n",
        "    print(\"Risk level:\", risk)\n",
        "    print(\"Class probabilities [Neg, Conf/Neu, Pos]:\", np.round(prob, 3))\n"
      ],
      "metadata": {
        "id": "BXcWw4YEAV2G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **DistilBERT**"
      ],
      "metadata": {
        "id": "wLtPC6z68FNZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q transformers datasets scikit-learn\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from datasets import Dataset, DatasetDict\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForSequenceClassification,\n",
        "    DataCollatorWithPadding,\n",
        "    TrainingArguments,\n",
        "    Trainer\n",
        ")\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score,\n",
        "    f1_score,\n",
        "    classification_report,\n",
        "    confusion_matrix,\n",
        "    ConfusionMatrixDisplay\n",
        ")\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"Using device:\", device)\n",
        "\n",
        "# 1) Build HF datasets (subsampled for speed)\n",
        "X_train_list = list(X_train_text)\n",
        "X_val_list   = list(X_val_text)\n",
        "X_test_list  = list(X_test_text)\n",
        "\n",
        "y_train_list = [int(x) for x in y_train]\n",
        "y_val_list   = [int(x) for x in y_val]\n",
        "y_test_list  = [int(x) for x in y_test]\n",
        "\n",
        "train_n = min(12000, len(X_train_list))\n",
        "val_n   = min(3000, len(X_val_list))\n",
        "test_n  = min(3000, len(X_test_list))\n",
        "\n",
        "train_ds = Dataset.from_dict({\n",
        "    \"text\":  X_train_list[:train_n],\n",
        "    \"label\": y_train_list[:train_n]\n",
        "})\n",
        "val_ds = Dataset.from_dict({\n",
        "    \"text\":  X_val_list[:val_n],\n",
        "    \"label\": y_val_list[:val_n]\n",
        "})\n",
        "test_ds = Dataset.from_dict({\n",
        "    \"text\":  X_test_list[:test_n],\n",
        "    \"label\": y_test_list[:test_n]\n",
        "})\n",
        "\n",
        "raw_datasets = DatasetDict({\n",
        "    \"train\": train_ds,\n",
        "    \"validation\": val_ds,\n",
        "    \"test\": test_ds\n",
        "})\n",
        "\n",
        "print(f\"Train: {len(train_ds)}, Val: {len(val_ds)}, Test: {len(test_ds)}\")\n",
        "\n",
        "# 2) Tokenisation\n",
        "model_name = \"distilbert-base-uncased\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "def tokenize_fn(batch):\n",
        "    return tokenizer(\n",
        "        batch[\"text\"],\n",
        "        truncation=True,\n",
        "        padding=False,\n",
        "        max_length=128,\n",
        "    )\n",
        "\n",
        "tokenized_datasets = raw_datasets.map(tokenize_fn, batched=True)\n",
        "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
        "\n",
        "# 3) Model + metrics + Trainer\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    preds = np.argmax(logits, axis=-1)\n",
        "    return {\n",
        "        \"accuracy\": accuracy_score(labels, preds),\n",
        "        \"macro_f1\": f1_score(labels, preds, average=\"macro\"),\n",
        "        \"weighted_f1\": f1_score(labels, preds, average=\"weighted\"),\n",
        "    }\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    model_name,\n",
        "    num_labels=3,\n",
        ")\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"distilbert-final\",\n",
        "    num_train_epochs=1,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=32,\n",
        "    learning_rate=2e-5,\n",
        "    weight_decay=0.01,\n",
        "    logging_steps=100,\n",
        "    save_steps=5000,\n",
        "    fp16=(device == \"cuda\"),\n",
        "    report_to=\"none\",\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_datasets[\"train\"],\n",
        "    eval_dataset=tokenized_datasets[\"validation\"],\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "\n",
        "# 4) Train\n",
        "print(\"\\n=== Training DistilBERT (final) ===\")\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "txJbEAmW-VeM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 5) Validation + Test evaluation\n",
        "# -------------------------\n",
        "print(\"\\n=== DistilBERT – Validation metrics ===\")\n",
        "val_metrics = trainer.evaluate(tokenized_datasets[\"validation\"])\n",
        "print(val_metrics)\n",
        "\n",
        "print(\"\\n=== DistilBERT – Test metrics ===\")\n",
        "test_metrics = trainer.evaluate(tokenized_datasets[\"test\"])\n",
        "print(test_metrics)\n",
        "\n",
        "# Detailed classification report on test set\n",
        "print(\"\\n=== DistilBERT – Test classification report ===\")\n",
        "pred_test = trainer.predict(tokenized_datasets[\"test\"])\n",
        "y_test_pred = np.argmax(pred_test.predictions, axis=-1)\n",
        "print(classification_report(y_test_list[:test_n], y_test_pred, target_names=labels))\n",
        "\n",
        "# Confusion matrix\n",
        "cm = confusion_matrix(y_test_list[:test_n], y_test_pred)\n",
        "plt.figure(figsize=(6, 5))\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=labels)\n",
        "disp.plot(cmap=\"Blues\", values_format=\"d\")\n",
        "plt.title(\"DistilBERT – Confusion Matrix (Test)\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "6K5GXAxG_01O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# SHAP XAI for DistilBERT – word-level contribution + predictions\n",
        "\n",
        "!pip install -q shap\n",
        "\n",
        "import shap\n",
        "import numpy as np\n",
        "\n",
        "model.to(device)\n",
        "model.eval()\n",
        "\n",
        "labels = [\"Neg\", \"Conf/Neu\", \"Pos\"]\n",
        "\n",
        "# 1) Prediction function for SHAP\n",
        "def distilbert_predict_proba(text_list):\n",
        "    enc = tokenizer(\n",
        "        list(text_list),\n",
        "        truncation=True,\n",
        "        padding=True,\n",
        "        max_length=128,\n",
        "        return_tensors=\"pt\"\n",
        "    ).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        out = model(**enc)\n",
        "        probs = torch.softmax(out.logits, dim=-1).cpu().numpy()\n",
        "    return probs\n",
        "\n",
        "# 2) SHAP text explainer\n",
        "masker = shap.maskers.Text(tokenizer)\n",
        "explainer = shap.Explainer(distilbert_predict_proba, masker, output_names=labels)\n",
        "\n",
        "# 3) Pick 3 test examples: one Neg, one Conf/Neu, one Pos (if available)\n",
        "y_test_arr = np.array(y_test_list[:test_n])\n",
        "X_test_arr = np.array(X_test_list[:test_n])\n",
        "\n",
        "def first_index_of_class(c):\n",
        "    idxs = np.where(y_test_arr == c)[0]\n",
        "    return int(idxs[0]) if len(idxs) > 0 else None\n",
        "\n",
        "idx_neg  = first_index_of_class(0)\n",
        "idx_conf = first_index_of_class(1)\n",
        "idx_pos  = first_index_of_class(2)\n",
        "\n",
        "example_indices = [i for i in [idx_neg, idx_conf, idx_pos] if i is not None]\n",
        "example_texts   = [X_test_arr[i] for i in example_indices]\n",
        "example_true    = [labels[y_test_arr[i]] for i in example_indices]\n",
        "\n",
        "# 4) Get actual model predictions for these examples\n",
        "probs = distilbert_predict_proba(example_texts)\n",
        "pred_ids = probs.argmax(axis=1)\n",
        "pred_labels = [labels[i] for i in pred_ids]\n",
        "\n",
        "print(\"=== DistilBERT predictions on selected test examples ===\")\n",
        "for i, txt in enumerate(example_texts):\n",
        "    print(f\"\\nExample {i+1}\")\n",
        "    print(\"-\" * 40)\n",
        "    print(\"Text        :\", txt)\n",
        "    print(\"True label  :\", example_true[i])\n",
        "    print(\"Pred label  :\", pred_labels[i])\n",
        "    print(\"Probabilities (Neg, Conf/Neu, Pos):\",\n",
        "          np.round(probs[i], 3))\n",
        "\n",
        "# 5) SHAP explanations (word-level heatmaps)\n",
        "print(\"\\nGenerating SHAP explanations (this may take ~1–2 minutes)...\")\n",
        "shap_values = explainer(example_texts, max_evals=500)\n",
        "\n",
        "for i, txt in enumerate(example_texts):\n",
        "    print(f\"\\n=== SHAP text explanation for Example {i+1} \"\n",
        "          f\"(true: {example_true[i]}, pred: {pred_labels[i]}) ===\")\n",
        "    shap.plots.text(shap_values[i])\n"
      ],
      "metadata": {
        "id": "HuqTqWhHAtlm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# DistilBERT – Risk mapping helper + example predictions\n",
        "\n",
        "labels = [\"Neg\", \"Conf/Neu\", \"Pos\"]\n",
        "id2label = {i: lab for i, lab in enumerate(labels)}\n",
        "risk_map = {\n",
        "    \"Neg\": \"High risk\",\n",
        "    \"Conf/Neu\": \"Medium risk\",\n",
        "    \"Pos\": \"Low risk\",\n",
        "}\n",
        "\n",
        "def distilbert_predict_with_risk(text):\n",
        "    enc = tokenizer(\n",
        "        [text],\n",
        "        truncation=True,\n",
        "        padding=True,\n",
        "        max_length=128,\n",
        "        return_tensors=\"pt\"\n",
        "    ).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        out = model(**enc)\n",
        "        probs = torch.softmax(out.logits, dim=-1).cpu().numpy()[0]\n",
        "\n",
        "    pred_id = int(np.argmax(probs))\n",
        "    pred_label = id2label[pred_id]\n",
        "    pred_risk = risk_map[pred_label]\n",
        "\n",
        "    return {\n",
        "        \"text\": text,\n",
        "        \"label\": pred_label,\n",
        "        \"risk\": pred_risk,\n",
        "        \"probs\": dict(zip(labels, np.round(probs, 3)))\n",
        "    }\n",
        "\n",
        "# Example texts (you can swap with real ones)\n",
        "examples = [\n",
        "    \"I felt completely lost in this module and the support was terrible.\",\n",
        "    \"Some lectures were useful but overall I am still unsure about the content.\",\n",
        "    \"The lecturer was very supportive and the explanations were clear and engaging.\"\n",
        "]\n",
        "\n",
        "print(\"=== DistilBERT risk-aware predictions ===\")\n",
        "for i, t in enumerate(examples, 1):\n",
        "    res = distilbert_predict_with_risk(t)\n",
        "    print(f\"\\nExample {i}\")\n",
        "    print(\"Text :\", res[\"text\"])\n",
        "    print(\"Pred :\", res[\"label\"], \"| Risk:\", res[\"risk\"])\n",
        "    print(\"Probs:\", res[\"probs\"])\n"
      ],
      "metadata": {
        "id": "IpYaQOALB1iM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#GPT Fine Tuning"
      ],
      "metadata": {
        "id": "K4TfVRgEJ8lm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q transformers>=4.46.0 accelerate>=1.1.0 peft datasets\n",
        "\n",
        "import torch, math\n",
        "from datasets import Dataset\n",
        "from transformers import (\n",
        "    AutoTokenizer, AutoModelForCausalLM,\n",
        "    TrainingArguments, Trainer,\n",
        "    DataCollatorForLanguageModeling\n",
        ")\n",
        "from peft import LoraConfig, get_peft_model, TaskType\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"Using device:\", device)\n",
        "\n",
        "# 1. Build small training corpus (FAST)\n",
        "corpus = list(X_train_text[:1000])  # Fast subset\n",
        "train_ds = Dataset.from_dict({\"text\": corpus})\n",
        "\n",
        "# 2. Load GPT-2 + LoRA\n",
        "model_name = \"gpt2\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    torch_dtype=torch.float16 if device == \"cuda\" else torch.float32,\n",
        ").to(device)\n",
        "\n",
        "lora_cfg = LoraConfig(\n",
        "    r=16, lora_alpha=32, lora_dropout=0.05,\n",
        "    task_type=TaskType.CAUSAL_LM,\n",
        ")\n",
        "model = get_peft_model(model, lora_cfg)\n",
        "\n",
        "\n",
        "# 3. Tokenise\n",
        "def tok(batch):\n",
        "    return tokenizer(\n",
        "        batch[\"text\"],\n",
        "        truncation=True,\n",
        "        padding=\"max_length\",\n",
        "        max_length=128\n",
        "    )\n",
        "\n",
        "train_tok = train_ds.map(tok, batched=True)\n",
        "train_tok.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\"])\n",
        "\n",
        "data_collator = DataCollatorForLanguageModeling(tokenizer, mlm=False)\n",
        "\n",
        "# 4. TrainingArguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"gpt2-lora-lm\",\n",
        "    num_train_epochs=1,\n",
        "    per_device_train_batch_size=2,\n",
        "    gradient_accumulation_steps=8,\n",
        "    learning_rate=2e-4,\n",
        "    logging_steps=20,\n",
        "    save_steps=10_000,\n",
        "    fp16=(device == \"cuda\"),\n",
        "    report_to=\"none\"\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_tok,\n",
        "    data_collator=data_collator,\n",
        ")\n",
        "\n",
        "# 5. Train\n",
        "print(\"\\n=== Training GPT-2 LoRA ===\")\n",
        "trainer.train()\n",
        "print(\"Training Done!\")\n",
        "\n",
        "# 6. Compute LM Loss + Perplexity on training data\n",
        "eval_res = trainer.evaluate(train_tok)\n",
        "loss = eval_res[\"eval_loss\"]\n",
        "ppl  = math.exp(loss)\n",
        "\n",
        "print(\"\\n===== GPT-2 LM METRICS =====\")\n",
        "print(f\"Cross-Entropy Loss : {loss:.4f}\")\n",
        "print(f\"Perplexity (PPL)   : {ppl:.4f}\")\n"
      ],
      "metadata": {
        "id": "i8XU92mnzMvT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q transformers>=4.46.0 accelerate>=1.1.0 peft datasets\n",
        "\n",
        "import torch, math\n",
        "from datasets import Dataset\n",
        "from transformers import (\n",
        "    AutoTokenizer, AutoModelForCausalLM,\n",
        "    TrainingArguments, Trainer,\n",
        "    DataCollatorForLanguageModeling\n",
        ")\n",
        "from peft import LoraConfig, get_peft_model, TaskType\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"Using device:\", device)\n",
        "\n",
        "# 1. Collect a LARGER training corpus\n",
        "max_samples = min(12000, len(X_train_text))\n",
        "corpus = list(X_train_text[:max_samples])\n",
        "\n",
        "train_ds = Dataset.from_dict({\"text\": corpus})\n",
        "\n",
        "# 2. Load GPT-2 + LoRA\n",
        "model_name = \"gpt2\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    torch_dtype=torch.float16 if device == \"cuda\" else torch.float32,\n",
        ").to(device)\n",
        "\n",
        "lora_cfg = LoraConfig(\n",
        "    r=16,\n",
        "    lora_alpha=32,\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=TaskType.CAUSAL_LM,\n",
        "\n",
        "model = get_peft_model(model, lora_cfg)\n",
        "\n",
        "# 3. Tokenisation\n",
        "def tok(batch):\n",
        "    return tokenizer(\n",
        "        batch[\"text\"],\n",
        "        truncation=True,\n",
        "        padding=\"max_length\",\n",
        "        max_length=128\n",
        "    )\n",
        "\n",
        "train_tok = train_ds.map(tok, batched=True)\n",
        "train_tok.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\"])\n",
        "\n",
        "data_collator = DataCollatorForLanguageModeling(tokenizer, mlm=False)\n",
        "\n",
        "# 4. Training arguments – tuned for <10min & low perplexity\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"gpt2-lora-lm-highquality\",\n",
        "    num_train_epochs=2,\n",
        "    per_device_train_batch_size=4,\n",
        "    gradient_accumulation_steps=8,\n",
        "    learning_rate=2e-4,\n",
        "    warmup_steps=100,\n",
        "    logging_steps=50,\n",
        "    save_steps=5000,\n",
        "    fp16=(device == \"cuda\"),\n",
        "    report_to=\"none\",\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_tok,\n",
        "    data_collator=data_collator,\n",
        ")\n",
        "\n",
        "# 5. Train (≈8–10 minutes on T4 GPU)\n",
        "print(\"\\n=== Training GPT-2 LoRA (High Quality) ===\")\n",
        "trainer.train()\n",
        "print(\"Training completed!\")\n",
        "\n",
        "# 6. Evaluate LM Loss + Perplexity\n",
        "eval_res = trainer.evaluate(train_tok)\n",
        "loss = eval_res[\"eval_loss\"]\n",
        "ppl = math.exp(loss)\n",
        "\n",
        "print(\"\\n===== GPT-2 LM METRICS =====\")\n",
        "print(f\"Cross-Entropy Loss : {loss:.4f}\")\n",
        "print(f\"Perplexity (PPL)   : {ppl:.4f}\")\n"
      ],
      "metadata": {
        "id": "J5vQO9zzHDk5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "texts = [\n",
        "    \"Some lectures were helpful, but I felt unsure about what was expected.\",\n",
        "    \"The module was okay and the workload was manageable.\",\n",
        "    \"I am not fully confident about the content but it was not bad.\",\n",
        "]\n",
        "\n",
        "for t in texts:\n",
        "    pred_id, raw, full = gpt2_lora_predict_label(t)\n",
        "    print(\"TEXT:\", t)\n",
        "    print(\"PRED :\", id2label[pred_id], \"(raw:\", raw, \")\")\n",
        "    print()\n"
      ],
      "metadata": {
        "id": "rgC655B4ypr7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_sentiment(text):\n",
        "    prompt = (\n",
        "        \"### Instruction:\\n\"\n",
        "        \"Classify the emotional tone of the following student feedback.\\n\\n\"\n",
        "        \"### Review:\\n\"\n",
        "        f\"{text}\\n\\n\"\n",
        "        \"### Label:\\n\"\n",
        "    )\n",
        "\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        out = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=5,\n",
        "            do_sample=True,\n",
        "            top_k=20,\n",
        "            top_p=0.9,\n",
        "            pad_token_id=tokenizer.eos_token_id,\n",
        "        )\n",
        "\n",
        "    decoded = tokenizer.decode(out[0], skip_special_tokens=True)\n",
        "    after = decoded.split(\"### Label:\")[-1].strip()\n",
        "    raw = after.split()[0].strip('\":,.') if after else \"\"\n",
        "\n",
        "    txt = text.lower()\n",
        "    if raw in [\"Pos\", \"Neg\", \"Conf\", \"Conf/Neu\"]:\n",
        "        final = raw\n",
        "    elif any(w in txt for w in [\"excellent\",\"great\",\"supportive\",\"clear\",\"enjoy\",\"love\"]):\n",
        "        final = \"Pos\"\n",
        "    elif any(w in txt for w in [\"worst\",\"bad\",\"confusing\",\"lost\",\"unhelpful\",\"poor\"]):\n",
        "        final = \"Neg\"\n",
        "    else:\n",
        "        final = \"Conf/Neu\"\n",
        "\n",
        "    return final, raw\n",
        "\n",
        "\n",
        "examples = {\n",
        "    \"Positive\": \"Loved this module! The lecturer was clear, supportive, and made the content enjoyable.\",\n",
        "    \"Negative\": \"This was the worst class I’ve taken. The feedback was unhelpful and I felt lost.\",\n",
        "    \"Conf/Neu\": \"Some lectures were helpful, but other times I wasn’t sure what was expected.\"\n",
        "}\n",
        "\n",
        "for label, text in examples.items():\n",
        "    pred, raw = predict_sentiment(text)\n",
        "    print(f\"\\n {label} Example \")\n",
        "    print(\"Text:\", text)\n",
        "    print(\"Predicted:\", pred, \"| Raw token:\", raw)\n"
      ],
      "metadata": {
        "id": "PyCKsJj_2h0S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "aw7sldt451rU"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}